{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y40wa0Sqcy-B"
      },
      "source": [
        "## COMPAS Analysis using Aequitas\n",
        "<a id='top_cell'></a>\n",
        "Recent work in the Machine Learning community has raised concerns about the risk of unintended bias in Algorithmic Decision-Making systems, affecting individuals unfairly. While many bias metrics and fairness definitions have been proposed in recent years, the community has not reached a consensus on which definitions and metrics should be used, and there has been very little empirical analyses of real-world problems using the proposed metrics.\n",
        "\n",
        "We present the Aequitas toolkit as an intuitive addition to the machine learning workflow, enabling users to to seamlessly test models for several bias and fairness metrics in relation to multiple population groups. We believe the tool will faciliate informed and equitable decision-making around developing and deploying predictive risk-assessment tools for both machine learnining practitioners and policymakers, allowing researchers and program managers to answer a host of questions related to machine learning models, including:\n",
        "\n",
        "- [What biases exist in my model?](#existing_biases)\n",
        "    - [What is the distribution of groups, predicted scores, and labels across my dataset?](#xtab)\n",
        "    - [What are bias metrics across groups?](#xtab_metrics)\n",
        "    - [How do I interpret biases in my model?](#interpret_bias)\n",
        "    - [How do I visualize biases in my model?](#bias_viz)\n",
        "\n",
        "- [What levels of disparity exist between population groups?](#disparities)\n",
        "    - [How does the selected reference group affect disparity calculations?](#disparity_calc)\n",
        "    - [How do I interpret calculated disparity ratios?](#interpret_disp)\n",
        "    - [How do I visualize disparities in my model?](#disparity_viz)\n",
        "\n",
        "- [How do I assess model fairness??](#fairness)\n",
        "    - [How do I interpret parities?](#interpret_fairness)\n",
        "    - [How do I visualize bias metric parity?](#fairness_group_viz)\n",
        "    - [How do I visualize parity between groups in my model?](#fairness_disp_viz)\n",
        "\n",
        "\n",
        "We apply the toolkit to the COMPAS dataset reported on by ProPublica below.\n",
        "\n",
        "### Background\n",
        "\n",
        "In 2016, ProPublica reported on racial inequality in automated criminal risk assessment algorithms. The [report](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) is based on [this analysis](https://github.com/propublica/compas-analysis). Using a clean version of the COMPAS dataset from the ProPublica GitHub repo, we demostrate the use of the Aequitas bias reporting tool.\n",
        "\n",
        "Northpointe's COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is one of the most widesly utilized risk assessment tools/ algorithms within the criminal justice system for guiding decisions such as how to set bail. The ProPublica dataset represents two years of COMPAS predicitons from Broward County, FL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:31.068996Z",
          "start_time": "2020-08-18T18:07:11.042342Z"
        },
        "id": "g30m7Cvlcy-T"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/dssg/aequitas.git@release-fixes &> /dev/null\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from aequitas.audit import Audit\n",
        "from aequitas.fairness import Fairness\n",
        "import aequitas.plot as ap\n",
        "\n",
        "# import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:31.135890Z",
          "start_time": "2020-08-18T18:07:31.071760Z"
        },
        "id": "JMgd_ARIcy-X",
        "outputId": "665eb6c7-b208-40db-b2a8-c58208d2f041"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://github.com/dssg/aequitas/raw/master/examples/data/compas_for_aequitas.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:31.157816Z",
          "start_time": "2020-08-18T18:07:31.142203Z"
        },
        "id": "NdEC8ygecy-c",
        "outputId": "be35fc7c-db86-452d-c04a-cec528864c60"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEqdozEccy-d"
      },
      "source": [
        "## Pre-Aequitas: Exploring the COMPAS Dataset\n",
        "\n",
        "__Risk assessment by race__\n",
        "\n",
        "COMPAS produces a risk score that predicts a person's likelihood of commiting a crime in the next two years. The output is a score between 1 and 10 that maps to low, medium or high. For Aequitas, we collapse this to a binary prediction. A score of 0 indicates a prediction of \"low\" risk according to COMPAS, while a 1 indicates \"high\" or \"medium\" risk.\n",
        "\n",
        "This categorization is based on ProPublica's interpretation of Northpointe's practioner guide:\n",
        "\n",
        "    \"According to Northpointe’s practitioners guide, COMPAS “scores in the medium and high range\n",
        "    garner more interest from supervision agencies than low scores, as a low score would suggest\n",
        "    there is little risk of general recidivism,” so we considered scores any higher than “low” to\n",
        "    indicate a risk of recidivism.\"\n",
        "\n",
        "In the bar charts below, we see a large difference in how these scores are distributed by race, with a majority of white and Hispanic people predicted as low risk (score = 0) and a majority of black people predicted high and medium risk (score = 1). We also see that while the majority of people in age categories over 25 are predicted as low risk (score = 0), the majority of people below 25 are predicted as high and medium risk (score = 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:31.199945Z",
          "start_time": "2020-08-18T18:07:31.173293Z"
        },
        "id": "urRoT5M1cy-f"
      },
      "outputs": [],
      "source": [
        "aq_palette = sns.diverging_palette(225, 35, n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:31.652368Z",
          "start_time": "2020-08-18T18:07:31.202936Z"
        },
        "id": "TDHSTIqPcy-g",
        "outputId": "b002c0eb-e584-4e14-dc3d-ed0137c62e04"
      },
      "outputs": [],
      "source": [
        "by_race = sns.countplot(x=\"race\", hue=\"score\", data=df[df.race.isin(['African-American', 'Caucasian', 'Hispanic'])], palette=aq_palette)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:31.950242Z",
          "start_time": "2020-08-18T18:07:31.655842Z"
        },
        "id": "_2_LOQprcy-o",
        "outputId": "9eb08394-89e5-4366-8745-f7b1a9a12093"
      },
      "outputs": [],
      "source": [
        "by_sex = sns.countplot(x=\"sex\", hue=\"score\", data=df, palette=aq_palette)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:32.226536Z",
          "start_time": "2020-08-18T18:07:31.953389Z"
        },
        "id": "dSzyBfM4cy-q",
        "outputId": "3592a3f4-c5e4-4e16-f2d1-7c652b84e463"
      },
      "outputs": [],
      "source": [
        "by_age = sns.countplot(x=\"age_cat\", hue=\"score\", data=df, palette=aq_palette)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BFWnPYZcy-s"
      },
      "source": [
        "__Levels of recidivism__\n",
        "\n",
        "This dataset includes information about whether or not the subject recidivated, and so we can directly test the accuracy of the predictions. First, we visualize the recidivsm rates across race.\n",
        "\n",
        "Following ProPublica, we defined recidivism as a new arrest within two years. (If a person recidivates, `label_value` = 1). They \"based this decision on Northpointe’s practitioners guide, which says that its recidivism score is meant to predict 'a new misdemeanor or felony offense within two years of the COMPAS administration date.'\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:32.534272Z",
          "start_time": "2020-08-18T18:07:32.234958Z"
        },
        "id": "QUIIOi9zcy-s",
        "outputId": "3520f3c8-1a29-4661-b031-511ba442e493"
      },
      "outputs": [],
      "source": [
        "label_by_race = sns.countplot(x=\"race\", hue=\"label_value\", data=df[df.race.isin(['African-American', 'Caucasian', 'Hispanic'])], palette=aq_palette)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:32.797684Z",
          "start_time": "2020-08-18T18:07:32.542814Z"
        },
        "id": "c1xEYQ49cy-t",
        "outputId": "7f5cf1bc-2be7-4fdb-d1e3-c706096e881d"
      },
      "outputs": [],
      "source": [
        "label_by_age = sns.countplot(x=\"sex\", hue=\"label_value\", data=df, palette=aq_palette)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:33.076512Z",
          "start_time": "2020-08-18T18:07:32.801014Z"
        },
        "id": "J24xyd68cy-u",
        "outputId": "8ebbd090-6543-4605-946d-7467dff6d487"
      },
      "outputs": [],
      "source": [
        "label_by_sex = sns.countplot(x=\"age_cat\", hue=\"label_value\", data=df, palette=aq_palette)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZWqpG7tcy-v"
      },
      "source": [
        "## Putting Aequitas to the task\n",
        "\n",
        "The graphs above show the base rates for recidivism are higher for black defendants compared to white defendants (.51 vs .39), though the predictions do not match the base rates.\n",
        "\n",
        "Practitioners face the challenge of determining whether or not such patterns reflect bias or not. The fact that there are multiple ways to measure bias adds complexity to the decision-making process. With Aequitas, we provide a tool that automates the reporting of various fairness metrics to aid in this process.\n",
        "\n",
        "Applying Aequitas progammatically is a three step process represented by three python classes:\n",
        "\n",
        "`Group()`: Define groups\n",
        "\n",
        "`Bias()`: Calculate disparities\n",
        "\n",
        "`Fairness()`: Assert fairness\n",
        "\n",
        "Each class builds on the previous one expanding the output DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeFO0UBRcy-v"
      },
      "source": [
        "### Data Formatting\n",
        "\n",
        "Data for this example was preprocessed for compatibility with Aequitas. **The Aequitas tool always requires a `score` column and requires a binary `label_value` column for supervised metrics**, (i.e., False Discovery Rate, False Positive Rate, False Omission Rate, and False Negative Rate).\n",
        "\n",
        "Preprocessing includes but is not limited to checking for mandatory `score` and `label_value` columns as well as at least one column representing attributes specific to the data set. See [documentation](../input_data.html) for more information about input data.\n",
        "\n",
        "Note that while `entity_id` is not necessary for this example, Aequitas recognizes `entity_id` as a reserve column name and will not recognize it as an attribute column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouqVNFI2cy-w"
      },
      "source": [
        "[Back to Top](#top_cell)\n",
        "<a id='existing_biases'></a>\n",
        "\n",
        "## What biases exist in my model?\n",
        "\n",
        "### _Aequitas Group() Class_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPjBizwdcy-w"
      },
      "source": [
        "<a id='xtab'></a>\n",
        "\n",
        "### What is the distribution of groups, predicted scores, and labels across my dataset?\n",
        "\n",
        "Aequitas's `Group()` class enables researchers to evaluate biases across all subgroups in their dataset by assembling a confusion matrix of each subgroup, calculating commonly used metrics such as false positive rate and false omission rate, as well as counts by group and group prevelance among the sample population."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eut4NhPxcy-w"
      },
      "source": [
        "<a id='counts_description'></a>\n",
        "The **`get_crosstabs()`** method tabulates a confusion matrix for each subgroup and calculates commonly used metrics such as false positive rate and false omission rate. It also provides counts by group and group prevelances.\n",
        "\n",
        "#### Group Counts Calculated:\n",
        "\n",
        "| Count Type | Column Name |\n",
        "| --- | --- |\n",
        "| False Positive Count | 'fp' |\n",
        "| False Negative Count | 'fn' |\n",
        "| True Negative Count | 'tn' |\n",
        "| True Positive Count | 'tp' |\n",
        "| Predicted Positive Count | 'pp' |\n",
        "| Predicted Negative Count | 'pn' |\n",
        "| Count of Negative Labels in Group | 'group_label_neg' |\n",
        "| Count of Positive Labels in Group | 'group_label_pos' |\n",
        "| Group Size | 'group_size'|\n",
        "| Total Entities | 'total_entities' |\n",
        "\n",
        "#### Absolute Metrics Calculated:\n",
        "\n",
        "| Metric | Column Name |\n",
        "| --- | --- |\n",
        "| True Positive Rate | 'tpr' |\n",
        "| True Negative Rate | 'tnr' |\n",
        "| False Omission Rate | 'for' |\n",
        "| False Discovery Rate | 'fdr' |\n",
        "| False Positive Rate | 'fpr' |\n",
        "| False Negative Rate | 'fnr' |\n",
        "| Negative Predictive Value | 'npv' |\n",
        "| Precision | 'precision' |\n",
        "| Predicted Positive Ratio$_k$ | 'ppr' |\n",
        "| Predicted Positive Ratio$_g$ | 'pprev' |\n",
        "| Group Prevalence | 'prev' |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sVu1dLbcy-x"
      },
      "source": [
        "**Note**: The **`get_crosstabs()`** method expects a dataframe with predefined columns `score`, and `label_value` and treats other columns (with a few exceptions) as attributes against which to test for disparities. In this case, we include `race`, `sex` and `age_cat`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:33.893916Z",
          "start_time": "2020-08-18T18:07:33.081651Z"
        },
        "id": "McaWFq8xcy-x"
      },
      "outputs": [],
      "source": [
        "audit = Audit(df.drop(columns=[\"entity_id\"]), label_column=\"label_value\")\n",
        "audit.audit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIboCjXdcy-x"
      },
      "source": [
        "[Back to Top](#top_cell)\n",
        "<a id='xtab_metrics'></a>\n",
        "\n",
        "### What are bias metrics across groups?\n",
        "\n",
        "Once you have run the `Group()` class **`get_crosstabs()`** method, you'll have a dataframe of the [group counts](#counts_description) and [group value bias metrics](#counts_description).\n",
        "\n",
        "The `Group()` class has a **`list_absolute_metrics()`** method, which you can use for faster slicing to view just  counts or bias metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExrArX8acy-y"
      },
      "source": [
        "#### View calculated counts across sample population groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:33.959132Z",
          "start_time": "2020-08-18T18:07:33.908654Z"
        },
        "id": "OgL0m1oxcy-y",
        "outputId": "7d5b0394-44fd-40f5-ff62-2da0d608b83a"
      },
      "outputs": [],
      "source": [
        "audit.confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B6BSIrccy-z"
      },
      "source": [
        "#### View calculated absolute metrics for each sample population group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:34.048216Z",
          "start_time": "2020-08-18T18:07:33.972812Z"
        },
        "id": "Dzkdo6J_cy-z",
        "outputId": "64d2f197-7cc9-402c-b732-7e10b4b28989"
      },
      "outputs": [],
      "source": [
        "audit.metrics.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkuL_xpzcy-0"
      },
      "source": [
        "[Back to Top](#top_cell)\n",
        "<a id='interpret_bias'></a>\n",
        "\n",
        "### How do I interpret biases in my model?\n",
        "In the slice of the crosstab dataframe created by the `Group()` class **`get_crosstabs()`** method directly above, we see that African-Americans have a false positive rate (`fpr`) of 45%, while Caucasians have a false positive rate of only 23%. This means that African-American people are far more likely to be falsely labeled as high-risk than white people. On the other hand, false ommision rates (`for`) and false discovery rates (`fdr`) are much closer for those two groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqlGadwccy-0"
      },
      "source": [
        "[Back to Top](#top_cell)\n",
        "<a id='disparities'></a>\n",
        "\n",
        "## What levels of disparity exist between population groups?\n",
        "\n",
        "### _Aequitas Bias() Class_\n",
        "We use the Aequitas `Bias()` class to calculate disparities between groups based on the crosstab returned by the `Group()` class **`get_crosstabs()`** method described above. Disparities are calculated as a ratio of a metric for a group of interest compared to a base group. For example, the False Negative Rate Disparity for black defendants vis-a-vis whites is:\n",
        "$$Disparity_{FNR} =  \\frac{FNR_{black}}{FNR_{white}}$$\n",
        "\n",
        "Below, we use **`get_disparity_predefined_groups()`** which allows us to choose reference groups that clarify the output for the practitioner.\n",
        "\n",
        "The Aequitas `Bias()` class includes two additional get disparity functions: **`get_disparity_major_group()`** and **`get_disparity_min_metric()`**, which automate base group selection based on sample majority (across each attribute) and minimum value for each calculated bias metric, respectively.  \n",
        "\n",
        "The **`get_disparity_predefined_groups()`** allows user to define a base group for each attribute, as illustrated below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnVmrvXicy-0"
      },
      "source": [
        "#### Disparities Calculated Calcuated:\n",
        "\n",
        "| Metric | Column Name |\n",
        "| --- | --- |\n",
        "| True Positive Rate Disparity | 'tpr_disprity' |\n",
        "| True Negative Rate | 'tnr_disparity' |\n",
        "| False Omission Rate | 'for_disparity' |\n",
        "| False Discovery Rate | 'fdr_disparity' |\n",
        "| False Positive Rate | 'fpr_disparity' |\n",
        "| False NegativeRate | 'fnr_disparity' |\n",
        "| Negative Predictive Value | 'npv_disparity' |\n",
        "| Precision Disparity | 'precision_disparity' |\n",
        "| Predicted Positive Ratio$_k$ Disparity | 'ppr_disparity' |\n",
        "| Predicted Positive Ratio$_g$ Disparity | 'pprev_disparity' |\n",
        "\n",
        "\n",
        "Columns for each disparity are appended to the crosstab dataframe, along with a column indicating the reference group for each calculated metric (denoted by `[METRIC NAME]_ref_group_value`). We see a slice of the dataframe with calculated metrics in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si74OXYpcy-5"
      },
      "source": [
        "#### Disparities calculated in relation to a user-specified group for each attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:39.798714Z",
          "start_time": "2020-08-18T18:07:38.214052Z"
        },
        "id": "PdrAi0Kycy-6",
        "outputId": "826383b1-7557-4588-8cba-ff4c14ae8e55"
      },
      "outputs": [],
      "source": [
        "audit = Audit(df.drop(columns=[\"entity_id\"]), label_column=\"label_value\",\n",
        "              reference_groups={'race':'Caucasian', 'sex':'Male', 'age_cat':'25 - 45'})\n",
        "\n",
        "audit.audit(bias_args={\n",
        "    \"alpha\": 0.05,\n",
        "    \"check_significance\": True,\n",
        "    \"mask_significance\": True\n",
        "})\n",
        "\n",
        "audit.disparity_df.style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1DYHZPGcy-7"
      },
      "source": [
        "The `Bias()` class includes a method to quickly return a list of calculated disparities from the dataframe returned by the **`get_disparity_`** methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:39.878536Z",
          "start_time": "2020-08-18T18:07:39.805271Z"
        },
        "id": "ZliPj_-Kcy-7",
        "outputId": "52be32c9-34bf-4de4-a85b-54b2455a8949"
      },
      "outputs": [],
      "source": [
        "audit.disparities.style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmJMtID3cy-7"
      },
      "outputs": [],
      "source": [
        "metrics = ['fpr','fdr']\n",
        "disparity_tolerance = 1.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siFtd3bUcy-8"
      },
      "source": [
        "# Aequitas Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKNKKIrmcy-8",
        "outputId": "52f5bd8d-aaff-4945-ddc6-bd9d6ebb940c"
      },
      "outputs": [],
      "source": [
        "audit.summary_plot(metrics=metrics, fairness_threshold=disparity_tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUz8UsAgcy-9"
      },
      "source": [
        "### Check for disparities in Race"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k4WhdMKcy-9",
        "outputId": "b3451632-91d1-4ca8-995d-fdffaedda26a"
      },
      "outputs": [],
      "source": [
        "audit.disparity_plot(metrics=metrics, attribute='race', fairness_threshold=disparity_tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE_4chrDcy_O"
      },
      "source": [
        "#### Check how the fairness threshold maps to the absolute values of each metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjUYKpxpcy_O",
        "outputId": "e9966a14-6843-4781-da42-6338d4281818"
      },
      "outputs": [],
      "source": [
        "ap.absolute(bdf, metrics, 'race', fairness_threshold = disparity_tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKVeUuPkcy_P"
      },
      "source": [
        "### Check for disparities in Sex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDncw6CIcy_P",
        "outputId": "4d73382f-9f86-45eb-f36c-4ccec971cd9d"
      },
      "outputs": [],
      "source": [
        "ap.disparity(bdf, metrics, 'sex', fairness_threshold = disparity_tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLPSlzpIcy_Q"
      },
      "source": [
        "#### Check how the fairness threshold maps to the absolute values of each metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DGBXFtrcy_Q",
        "outputId": "65511b69-8b82-469c-b2f3-cfc9fee2273a"
      },
      "outputs": [],
      "source": [
        "ap.absolute(bdf, metrics, 'sex', fairness_threshold = disparity_tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPyp5p2hcy_R"
      },
      "source": [
        "### Check for disparities in Age Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMq2UnnZcy_R",
        "outputId": "27393e2a-4b3a-49fe-9fa8-f2f88eefa2f9"
      },
      "outputs": [],
      "source": [
        "ap.disparity(bdf, metrics, 'age_cat', fairness_threshold = disparity_tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFlF76XScy_S"
      },
      "source": [
        "#### Check how the fairness threshold maps to the absolute values of each metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRTObtFjcy_S",
        "outputId": "f62ff509-0f85-421a-eb51-7430bb0b9d29"
      },
      "outputs": [],
      "source": [
        "ap.absolute(bdf, metrics, 'age_cat', fairness_threshold = disparity_tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p-vxaLvcy_T"
      },
      "source": [
        "[Back to Top](#top_cell)\n",
        "<a id='interpret_disp'></a>\n",
        "\n",
        "### How do I interpret calculated disparity ratios?\n",
        "The calculated disparities from the dataframe returned by the `Bias()` class **`get_disparity_`** methods are in relation to a reference group, which will always have a disparity of 1.0.\n",
        "\n",
        "The differences in False Positive Rates, noted in the discussion of the `Group()` class above, are clarified using the disparity ratio (`fpr_disparity`). Black people are falsely identified as being high or medium risks 1.9 times the rate for white people.\n",
        "\n",
        "As seen above, False Discovery Rates have much less disparity (`fdr_disparity`), or fraction of false postives over predicted positive in a group. As reference groups have disparity = 1 by design in Aequitas, the lower disparity is highlighted by the `fdr_disparity` value close to 1.0 (0.906) for the race attribute group 'African-American' when disparities are calculated using predefined base group 'Caucasian'. Note that COMPAS is calibrated to  balance False Positive Rate and False Discovery Rates across groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWpTd3tbcy_T"
      },
      "source": [
        "[Back to Top](#top_cell)\n",
        "<a id='disparity_calc'></a>\n",
        "\n",
        "### How does the selected reference group affect disparity calculations?\n",
        "\n",
        "Disparities calculated in the the Aequitas `Bias()` class based on the crosstab returned by the `Group()` class **`get_crosstabs()`** method can be derived using several different base gorups. In addition to using user-specified groups illustrated above, Aequitas can automate base group selection based on dataset characterisitcs:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YegvwdeScy_U"
      },
      "source": [
        "#### Evaluating disparities calculated in relation to a different 'race' reference group\n",
        "Changing even one attribute in the predefined groups will alter calculated disparities. When a differnet pre-defined group 'Hispanic' is used, we can see that Black people are 2.1 times more likely to be falsely identified as being high or medium risks as Hispanic people are (compared to 1.9 times more likely than white people), and even less likely to be falsely identified as low risk when compared to Hispanic people rather than white people."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:40.575125Z",
          "start_time": "2020-08-18T18:07:39.895024Z"
        },
        "id": "5VDHdNvjcy_U",
        "outputId": "ae8227b5-a91f-4ea6-ef1b-e3b44208b944",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "hbdf = b.get_disparity_predefined_groups(xtab, original_df=df,\n",
        "                                         ref_groups_dict={'race':'Hispanic', 'sex':'Male', 'age_cat':'25 - 45'},\n",
        "                                         alpha=0.05,\n",
        "                                         check_significance=True,\n",
        "                                         mask_significance=False,\n",
        "                                         selected_significance=['fpr', 'for', 'fdr'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:40.628241Z",
          "start_time": "2020-08-18T18:07:40.582059Z"
        },
        "id": "JKs8_-2jcy_V",
        "outputId": "5f371d80-701a-43b5-a815-aa045c604e8e"
      },
      "outputs": [],
      "source": [
        "# View disparity metrics added to dataframe\n",
        "hbdf[['attribute_name', 'attribute_value'] +\n",
        "     b.list_disparities(hbdf) + b.list_significance(hbdf)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WWo-DaZcy_V"
      },
      "source": [
        "#### Disparities calculated in relation to sample population majority group (in terms of group prevalence) for each attribute\n",
        "The majority population groups for each attribute ('race', 'sex', 'age_cat') in the COMPAS dataset are 'African American', 'Male', and '25 - 45'. Using the **`get_disparity_major_group()`** method of calculation allows researchers to quickly evaluate how much more (or less often) other groups are falsely or correctly identified as high- or medium-risk in relation to the group they have the most data on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:40.716625Z",
          "start_time": "2020-08-18T18:07:40.634860Z"
        },
        "id": "c7DLOI5Vcy_W",
        "outputId": "fc1b3f13-fdf7-4fee-fbb4-ea39180917a9"
      },
      "outputs": [],
      "source": [
        "majority_bdf = b.get_disparity_major_group(xtab, original_df=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:40.776266Z",
          "start_time": "2020-08-18T18:07:40.733565Z"
        },
        "id": "76YOdFwgcy_W",
        "outputId": "aa50d0d0-53f1-483f-8402-dc4b7fc95942"
      },
      "outputs": [],
      "source": [
        "majority_bdf[['attribute_name', 'attribute_value'] +  b.list_disparities(majority_bdf)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-uYl63scy_X"
      },
      "source": [
        "#### Disparities calculated in relation to the minimum value for each metric\n",
        "\n",
        "When you do not have a pre-existing don’t frame of reference or policy context for the dataset (ex: Caucasians or males historically favored), you may choose to view disparities in relation to the group with the lowest value for every disparity metric, as then every group's value will be at least 1.0, and relationships can be evaluated more linearly.\n",
        "\n",
        "\n",
        "Note that disparities are much more varied, and may have larger magnitude, when the minimum value per metric is used as a reference group versus one of the other two methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:42.712426Z",
          "start_time": "2020-08-18T18:07:40.793131Z"
        },
        "id": "07ISgWgucy_Y",
        "outputId": "37daec11-13d1-45c9-89c5-b6cbb721ed76"
      },
      "outputs": [],
      "source": [
        "min_metric_bdf = b.get_disparity_min_metric(df=xtab, original_df=df,\n",
        "                                            check_significance=True)\n",
        "min_metric_bdf.style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvNzeN4xcy_Z"
      },
      "source": [
        "[Back to Top](#top_cell)\n",
        "<a id='disparity_viz'></a>\n",
        "\n",
        "## How do I visualize disparities in my model?\n",
        "To visualize disparities in the dataframe returned by one of the `Bias()` class **`get_disparity_`** methods use one of two methods in the Aequitas `Plot()` class:\n",
        "\n",
        "A particular disparity metric can be specified with **`plot_disparity()`**. To plot a single disparity, a metric and an attribute must be specified.\n",
        "\n",
        "Disparities related to a list of particular metrics of interest or `'all'` metrics can be plotted with **`plot_disparity_all()`**.  At least one metric or at least one attribute must be specified when plotting multiple disparities (or the same disparity across multiple attributes). For example, to plot PPR and and Precision disparity for all attributes, specify `metrics=['ppr', 'precision']` with no attribute specified, and to plot default metrics by race, specify `attributes=['race']` and with no metrics specified.\n",
        "\n",
        "**Reference groups are displayed in grey, and always have a disparity = 1.** Note that disparities greater than 10x reference group will are visualized as 10x, and disparities less than 0.1x reference group are visualized as 0.1x.\n",
        "\n",
        "Statistical siginificance (at a default value of 0.05) is denoted by two asterisks (**) next to a treemap square's value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:54.322398Z",
          "start_time": "2020-08-18T18:07:54.279574Z"
        },
        "id": "pY0y8RF5cy_Z"
      },
      "outputs": [],
      "source": [
        "f = Fairness()\n",
        "fdf = f.get_group_value_fairness(bdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxDy56Sjcy_a"
      },
      "source": [
        "The `Fairness()` class includes a method to quickly return a list of fairness determinations from the dataframe returned by the **`get_group_value_fairness()`** method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:54.353196Z",
          "start_time": "2020-08-18T18:07:54.325343Z"
        },
        "id": "9KQvN-Hhcy_a"
      },
      "outputs": [],
      "source": [
        "parity_detrminations = f.list_parities(fdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:54.409313Z",
          "start_time": "2020-08-18T18:07:54.359723Z"
        },
        "id": "QtqS8wbAcy_b",
        "outputId": "027b00de-6d4b-453e-b1e6-edd33fed21a8"
      },
      "outputs": [],
      "source": [
        "fdf[['attribute_name', 'attribute_value'] + absolute_metrics + b.list_disparities(fdf) + parity_detrminations].style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmFEHvRTcy_b"
      },
      "source": [
        "[Back to Top](#top_cell)\n",
        "<a id='interpret_fairness'></a>\n",
        "\n",
        "### How do I interpret parities?\n",
        "Calling the Aequitas `Fairness()` class **`get_group_value_fairness()`** method on the dataframe returned from a `Bias()` class `get_dispariy` method will return the dataframe with additional columns indicating parities, as seen in the slice of the `get_group_value_fairness` data frame directly above.\n",
        "\n",
        "In this case, our base groups are Caucasian for race, Male for gender, and 25-45 for age_cat. By construction, the base group has supervised fairness. (The disparity ratio is 1). Relative to the base groups, the COMPAS predictions only provide supervised fairness to one group, Hispanic.\n",
        "\n",
        "Above, the African-American false omission and false discovery are within the bounds of fairness. This result is expected because COMPAS is calibrated. (Given calibration, it is surprising that Asian and Native American rates are so low. This may be a matter of having few observations for these groups.)\n",
        "\n",
        "On the other hand, African-Americans are roughly twice as likely to have false positives and 40 percent less likely to false negatives. In real terms, 44.8% of African-Americans who did not recidivate were marked high or medium risk (with potential for associated penalties), compared with 23.4% of Caucasian non-reoffenders. This is unfair and is marked False below.\n",
        "\n",
        "These findings mark an inherent trade-off between FPR Fairness, FNR Fairness and calibration, which is present in any decision system where base rates are not equal. See [Chouldechova (2017)](https://www.andrew.cmu.edu/user/achoulde/files/disparate_impact.pdf). Aequitas helps bring this trade-off to the forefront with clear metrics and asks system designers to make a reasoned decision based on their use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l37bJDlcy_c"
      },
      "source": [
        "### Attribute Level Fairness\n",
        "Use the **`get_group_attribute_fairness()`** function to view only the calculated parities from the **`get_group_value_fairness()`** function at the attribute level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:54.852857Z",
          "start_time": "2020-08-18T18:07:54.414166Z"
        },
        "id": "o2wp9Ajccy_c",
        "outputId": "89613c85-e0c9-411f-c264-93d9f619ad49"
      },
      "outputs": [],
      "source": [
        "gaf = f.get_group_attribute_fairness(fdf)\n",
        "gaf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89qS4gyBcy_c"
      },
      "source": [
        "### Overall Fairness\n",
        "The **`get_overall_fairness()`** function gives a quick boolean assessment of the output of **`get_group_value_fairness()`** or **`get_group_attribute_fairness()`**, returning a dictionary with a determination across all attributes for each of:\n",
        "- Unsupervised Fairness\n",
        "- Supervised Fairness\n",
        "- Overall Fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:54.881022Z",
          "start_time": "2020-08-18T18:07:54.874115Z"
        },
        "id": "xk2yzKTscy_d",
        "outputId": "073977dc-7987-42dc-a705-811a6b25687f"
      },
      "outputs": [],
      "source": [
        "gof = f.get_overall_fairness(fdf)\n",
        "gof"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RYxTXPqcy_d"
      },
      "source": [
        "## The Aequitas Effect\n",
        "\n",
        "By breaking down the COMPAS predictions using a variety of bias and disparity metrics calculated using different reference groups, we are able to surface the specific metrics for which the model is imposing bias on given attribute groups, and have a clearer lens when evaluating models and making recommendations for intervention.\n",
        "\n",
        "Researchers utilizing Aequitas will be able to make similar evaluations on their own data sets, and as they continue to use the tool, will begin to identify patterns in where biases exist and which models appear to produce less bias, thereby helping to reduce bias and its effects in future algorithm-based decision-making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu8n3YdQcy_d"
      },
      "source": [
        "[Back to Top](#top_cell)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
