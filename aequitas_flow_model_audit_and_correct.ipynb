{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsqKO--9ksr5"
      },
      "source": [
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/dssg/aequitas/master/src/aequitas_webapp/static/images/aequitas_flow_header.svg\"></center>\n",
        "\n",
        "# Correcting the predictions of a Model\n",
        "\n",
        "In this notebook we will first **load a Machine Learning model** created through an **`Experiment`** of **Aequitas Flow**. We will  measure its performance and run a **fairness audit**  using the application-specific configurations.\n",
        "\n",
        "We will then apply a **post-processing method to correct the predictions**, and observe any **changes in fairness** and **performance**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wDrvM8_n5mv"
      },
      "source": [
        "---\n",
        "## Initial Setup\n",
        "\n",
        "This section covers the initial setup required for the notebook. We'll be **installing the most recent version of Aequitas**.\n",
        "\n",
        "> ⚠️ **This notebook assumes that an ML Model has already been trained**. ⚠️\n",
        "\n",
        "We'll also be retrieving the model pickle file from a previous experiment, downloading it directly from the [Aequitas Repository](https://github.com/dssg/aequitas/tree/master/examples). However, the notebook supports the use of other models or datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPDql-NObJop"
      },
      "outputs": [],
      "source": [
        "# Install Aequitas (change to version install after release)\n",
        "!pip install git+https://github.com/dssg/aequitas.git@release-fixes &> /dev/null\n",
        "# This only needs to run once, or after your runtime environment gets deleted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KK264RHq2CU"
      },
      "outputs": [],
      "source": [
        "# This will avoid double logging in Colab\n",
        "from aequitas.flow.utils.logging import clean_handlers\n",
        "\n",
        "clean_handlers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsmu0T87q6Bx",
        "outputId": "e50491de-e81f-4f36-8676-462ce6b7e049"
      },
      "outputs": [],
      "source": [
        "# This cell will download a model from the repository. You do not need to run it if you have your won model.\n",
        "from aequitas.flow.utils.colab import get_examples\n",
        "\n",
        "get_examples(\"experiment_results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGTwdp8Eo_Cg"
      },
      "source": [
        "---\n",
        "## Loading the model & datasets\n",
        "\n",
        "In this section we will load the model for the audit and the evaluation datasets.\n",
        "\n",
        "If you are testing your own model, make sure to send it to the Colab environment (or any other environment you are using this notebook on).\n",
        "\n",
        "Starting with the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDl_KSJZrGUL"
      },
      "outputs": [],
      "source": [
        "pickle_path = \"examples/experiment_results/lgbm_baf_sample.pickle\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpPU4HfO3UgA"
      },
      "outputs": [],
      "source": [
        "# Change this cell if your model is loaded in a different form.\n",
        "import pickle\n",
        "\n",
        "with open(pickle_path, \"rb\") as f:\n",
        "    model = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJM2rtif3MrQ"
      },
      "source": [
        "In this example, we are using a sample of the **BankAccountFraud** dataset. This dataset presents a predictive task of detecting fraudulent attempts of bank account opening.\n",
        "\n",
        "In case you want to use a different dataset, make sure it is loaded as a pandas dataframe. If possible, configure an `aequitas.flow.datasets.GenericDataset`, for less changes in other cells.\n",
        "\n",
        "Now we will load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNw7H0gEdnSZ",
        "outputId": "805ff25f-6925-4764-fb63-40340cce0646"
      },
      "outputs": [],
      "source": [
        "from aequitas.flow.methods.base_estimator import LightGBM\n",
        "from aequitas.flow.datasets import BankAccountFraud\n",
        "\n",
        "dataset = BankAccountFraud(\"Sample\")\n",
        "dataset.load_data()\n",
        "dataset.create_splits()\n",
        "\n",
        "validation = dataset.validation\n",
        "test = dataset.test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho6YLarf4rVJ"
      },
      "source": [
        "---\n",
        "## Obtaining the predictions and thresholding the model\n",
        "\n",
        "In the following cells, we will generate the predictions with the model and the dataset, and create a threshold for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ1xMZ6I4YQd"
      },
      "outputs": [],
      "source": [
        "# If your model is not from\n",
        "preds_val = model.predict_proba(validation.X, validation.s)\n",
        "preds_test = model.predict_proba(test.X, test.s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWRvvenC9XEf"
      },
      "source": [
        "We use the `Threshold` from Aequitas Flow for thresholding. In the specific use-case of the BankAccountFraud dataset, we are pointing for a positive prediciton rate of 5%. You can adjust that by changing the instantiation of this class.\n",
        "\n",
        "The Threshold will be fitted to a validation set, and used to binarize the test set. Note that you can threshold the same test you fit, but this might lead to overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S81IqlG3ozML",
        "outputId": "960f4555-3b2a-4e79-a042-f31141fced13"
      },
      "outputs": [],
      "source": [
        "from aequitas.flow.methods.postprocessing import Threshold\n",
        "\n",
        "# We will create a threshold based to obtain 5% FPR on validation\n",
        "threshold = Threshold(threshold_type=\"top_pct\", threshold_value=0.05)\n",
        "threshold.fit(validation.X, preds_val, validation.y, validation.s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmR4q725p5uQ",
        "outputId": "ec9c6ce6-30ca-4caf-923f-2f95c0ffaa6e"
      },
      "outputs": [],
      "source": [
        "# Binarize test predictions with previously calculated threshold\n",
        "bin_preds_test = threshold.transform(test.X, preds_test, test.s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROkY2ALE-hl3"
      },
      "source": [
        "---\n",
        "## Fairness Audit and Performance Evaluation\n",
        "\n",
        "Now, we will create the resources necessary to perform a fairness audit, and evaluate the model performance. These are:\n",
        "1. **Protected attribute**\n",
        "2. **Model predictions**\n",
        "3. **Labels**\n",
        "\n",
        "But first, we will have to define some configurations of this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj9V5Hj4BQWc"
      },
      "source": [
        "As a brief summary of the task of the BankAccountFraud dataset, the performance is determined by the percentage of positive instances (frauds) detected (TPR). False Positives will incur in a non-fraudulent individual not having a bank account due to a false flag. Because of this, we want to equalize the rate of false positives (FPR) in relation to the protected attribute, in this case the customer age. The reference group for this task is the group with younger age (<50).\n",
        "\n",
        "> ⚠️ **Make sure to update the following configuration cell with the appropriate values for your use-case**. ⚠️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPAaigjzArnO"
      },
      "outputs": [],
      "source": [
        "performance_metric = \"tpr\"\n",
        "fairness_metric = \"fpr\"\n",
        "\n",
        "# The column name of the sensitive attribute can be obtained from the aequitas.flow.Dataset object\n",
        "fairness_column = test.s.name\n",
        "# The reference group for the example of BAF is \"0\", i.e. individuals from the younger group (<50).\n",
        "reference_group = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXZfmWuYn4M6"
      },
      "outputs": [],
      "source": [
        "# Creating a dataframe for the fairness audit\n",
        "audit_df = test.s.astype(str).to_frame()\n",
        "audit_df[\"label\"] = test.y\n",
        "# These might need to change if you are not using an aequitas.flow.Dataset object\n",
        "\n",
        "audit_df[\"score\"] = bin_preds_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaM-U7_HcOlO"
      },
      "source": [
        "In the cell bellow we will see the minimal structure for a fairness audit DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wg8YZRKxCZdJ",
        "outputId": "5bddcb42-a2eb-433a-9e7c-86f1c4ec217c"
      },
      "outputs": [],
      "source": [
        "audit_df.sample(n=5, random_state=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNH9djTU_7mg"
      },
      "source": [
        "We will quickly observe the performance of the model with a method for the effect.\n",
        "\n",
        "In this dataset, we are using global **TPR** as performance metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AlL6HKJroKk",
        "outputId": "22ed0a94-796d-447d-aa74-36282e675642"
      },
      "outputs": [],
      "source": [
        "from aequitas.audit import Audit\n",
        "\n",
        "audit = Audit(audit_df, reference_groups={fairness_column: reference_group})\n",
        "audit.performance()[performance_metric]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqb4jcJ3ATxi"
      },
      "source": [
        "We will now perform the fairness audit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "_P_7XVnz5f5S",
        "outputId": "b4890bcb-359c-44f8-b29c-4770e4459652"
      },
      "outputs": [],
      "source": [
        "audit.audit()\n",
        "audit.summary_plot(fairness_metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "2Z39au5opa3l",
        "outputId": "cfb0fd7d-9872-437e-9262-6d595d470b12"
      },
      "outputs": [],
      "source": [
        "audit.disparity_plot(fairness_metric, fairness_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "Gl4USnW9Ac-s",
        "outputId": "da4397ef-853e-4c2e-e3cf-889e03aef8e0"
      },
      "outputs": [],
      "source": [
        "audit.metrics_df[[\"attribute_name\", \"attribute_value\", \"tpr\", \"fpr\", \"accuracy\", \"precision\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2bNI3aKCrQg"
      },
      "source": [
        "---\n",
        "## Correcting the predictions\n",
        "\n",
        "To correct the predictions, we will use a method available in Aequitas Flow.\n",
        "\n",
        "This method will calculate different thresholds to equalize a target metric for all the groups (in this case the fairness metric, FPR)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPyuJ3cH7Dak",
        "outputId": "40a0edc2-a12d-48d7-d094-d5f7e66f404d"
      },
      "outputs": [],
      "source": [
        "from aequitas.flow.methods.postprocessing import BalancedGroupThreshold\n",
        "\n",
        "threshold = BalancedGroupThreshold(threshold_type=\"top_pct\", threshold_value=0.05, fairness_metric=\"fpr\")\n",
        "\n",
        "threshold.fit(validation.X, preds_val, validation.y, validation.s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zjd5v-C9iII",
        "outputId": "b14ca3bc-c061-46b7-ab14-6113b0f49227"
      },
      "outputs": [],
      "source": [
        "corrected_bin_preds_test = threshold.transform(test.X, preds_test, test.s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oCrnkkuuVEr"
      },
      "outputs": [],
      "source": [
        "audit_df = test.s.astype(str).to_frame().copy()\n",
        "audit_df[\"score\"] = corrected_bin_preds_test\n",
        "audit_df[\"label\"] = test.y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaGxbM_u_kOT"
      },
      "source": [
        "Let's see the impact of this correction in the global recall of the model, with the updated binarized predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAs_u5rXuPLz",
        "outputId": "eac87cdb-9ce1-4313-f977-ea808e3488b8"
      },
      "outputs": [],
      "source": [
        "from aequitas.audit import Audit\n",
        "\n",
        "audit_fixed = Audit(audit_df, reference_groups={fairness_column: reference_group})\n",
        "audit_fixed.performance()[performance_metric]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WYKciu6_x6y"
      },
      "source": [
        "When compared to the previous value of recall, there is no drop in performance.\n",
        "\n",
        "Let's observe the fairness audit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "YoWIUAhm-WXU",
        "outputId": "5ab80ae9-e8db-4512-c89a-d7ffe734e4b5"
      },
      "outputs": [],
      "source": [
        "audit_fixed.audit()\n",
        "audit_fixed.summary_plot(fairness_metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "wVA6NSSbp7at",
        "outputId": "30213074-71ac-43c5-9988-f83f082e789e"
      },
      "outputs": [],
      "source": [
        "audit_fixed.disparity_plot(fairness_metric, fairness_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "EVh1lFbCAw_h",
        "outputId": "681841f4-b3cb-4bf2-9b5c-3fdec7a2a0c6"
      },
      "outputs": [],
      "source": [
        "audit_fixed.metrics_df[[\"attribute_name\", \"attribute_value\", \"tpr\", \"fpr\", \"accuracy\", \"precision\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYK6cEktbQmK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
