<!DOCTYPE html>
<html lang="en"
      data-content_root="../../../"
      x-data="{ darkMode: localStorage.getItem('darkMode') || localStorage.setItem('darkMode', 'system'), activeSection: '' }"
      x-init="$watch('darkMode', val => localStorage.setItem('darkMode', val))"
      class="scroll-smooth"
      :class="{'dark': darkMode === 'dark' || (darkMode === 'system' && window.matchMedia('(prefers-color-scheme: dark)').matches)}"
>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta charset="utf-8" />
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="white" />
  <meta name="theme-color" metia="(prefers-color-scheme: dark)" content="black" />
  
    <title>src.aequitas.bias | aequitas  documentation</title>
    <meta property="og:title" content="src.aequitas.bias | aequitas  documentation" />
    <meta name="twitter:title" content="src.aequitas.bias | aequitas  documentation" />
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../_static/theme.css?v=5b4133db" />
        <link rel="search" title="Search" href="../../../search.html" />
        <link rel="index" title="Index" href="../../../genindex.html" />

    <script>
    <!-- Prevent Flash of wrong theme -->
      const userPreference = localStorage.getItem('darkMode');
      let mode;
      if (userPreference === 'dark' || window.matchMedia('(prefers-color-scheme: dark)').matches) {
        mode = 'dark';
        document.documentElement.classList.add('dark');
      } else {
        mode = 'light';
      }
      if (!userPreference) {localStorage.setItem('darkMode', mode)}
    </script>
<link href="../../../ _static/default.css" rel="stylesheet" type="text/css">

</head>
<body x-data="{ showSidebar: false }" class="min-h-screen font-sans antialiased bg-background text-foreground" :class="{ 'overflow-hidden': showSidebar }">
    <div x-cloak x-show="showSidebar" class="fixed inset-0 z-50 overflow-hidden bg-background/80 backdrop-blur-sm md:hidden" @click.self="showSidebar = false"></div><div id="page" class="relative flex flex-col min-h-screen"><a href="#content" class="absolute top-0 left-0 z-[100] block bg-background p-4 text-xl transition -translate-x-full opacity-0 focus:translate-x-0 focus:opacity-100">
      Skip to content
    </a><header
  class="sticky top-0 z-40 w-full border-b shadow-sm border-border supports-backdrop-blur:bg-background/60 bg-background/95 backdrop-blur"><div class="container flex items-center h-14">
    <div class="hidden mr-4 md:flex">
      <a href="../../../index.html" class="flex items-center mr-6"><span class="hidden font-bold sm:inline-block text-clip whitespace-nowrap">aequitas  documentation</span>
      </a></div><button
      class="inline-flex items-center justify-center h-10 px-0 py-2 mr-2 text-base font-medium transition-colors rounded-md hover:text-accent-foreground hover:bg-transparent md:hidden"
      type="button" @click="showSidebar = true">
      <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" aria-hidden="true"
        fill="currentColor">
        <path
          d="M152.587 825.087q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440Zm0-203.587q-19.152 0-32.326-13.174T107.087 576q0-19.152 13.174-32.326t32.326-13.174h320q19.152 0 32.326 13.174T518.087 576q0 19.152-13.174 32.326T472.587 621.5h-320Zm0-203.587q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440ZM708.913 576l112.174 112.174q12.674 12.674 12.674 31.826t-12.674 31.826Q808.413 764.5 789.261 764.5t-31.826-12.674l-144-144Q600 594.391 600 576t13.435-31.826l144-144q12.674-12.674 31.826-12.674t31.826 12.674q12.674 12.674 12.674 31.826t-12.674 31.826L708.913 576Z" />
      </svg>
      <span class="sr-only">Toggle navigation menu</span>
    </button>
    <div class="flex items-center justify-between flex-1 space-x-2 sm:space-x-4 md:justify-end">
      <div class="flex-1 w-full md:w-auto md:flex-none"><form id="searchbox"
      action="../../../search.html"
      method="get"
      class="relative flex items-center group"
      @keydown.k.window.meta="$refs.search.focus()">
  <input x-ref="search"
          name="q"
          id="search-input"
          type="search"
          aria-label="Search the docs"
          placeholder="Search ..."
          class="inline-flex items-center font-medium transition-colors bg-transparent focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 ring-offset-background border border-input hover:bg-accent focus:bg-accent hover:text-accent-foreground focus:text-accent-foreground hover:placeholder-accent-foreground py-2 px-4 relative h-9 w-full justify-start rounded-[0.5rem] text-sm text-muted-foreground sm:pr-12 md:w-40 lg:w-64" />
  <kbd class="pointer-events-none absolute right-1.5 top-2 hidden h-5 select-none text-muted-foreground items-center gap-1 rounded border border-border bg-muted px-1.5 font-mono text-[10px] font-medium opacity-100 sm:flex group-hover:bg-accent group-hover:text-accent-foreground">
    <span class="text-xs">âŒ˜</span>
    K
  </kbd>
</form>
      </div>
      <nav class="flex items-center space-x-1">
        <button @click="darkMode = darkMode === 'light' ? 'dark' : 'light'"
          class="relative inline-flex items-center justify-center px-0 text-sm font-medium transition-colors rounded-md hover:bg-accent hover:text-accent-foreground h-9 w-9"
          type="button">
          <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
            class="absolute transition-all scale-100 rotate-0 dark:-rotate-90 dark:scale-0">
            <path
              d="M480 685q45.456 0 77.228-31.772Q589 621.456 589 576q0-45.456-31.772-77.228Q525.456 467 480 467q-45.456 0-77.228 31.772Q371 530.544 371 576q0 45.456 31.772 77.228Q434.544 685 480 685Zm0 91q-83 0-141.5-58.5T280 576q0-83 58.5-141.5T480 376q83 0 141.5 58.5T680 576q0 83-58.5 141.5T480 776ZM80 621.5q-19.152 0-32.326-13.174T34.5 576q0-19.152 13.174-32.326T80 530.5h80q19.152 0 32.326 13.174T205.5 576q0 19.152-13.174 32.326T160 621.5H80Zm720 0q-19.152 0-32.326-13.174T754.5 576q0-19.152 13.174-32.326T800 530.5h80q19.152 0 32.326 13.174T925.5 576q0 19.152-13.174 32.326T880 621.5h-80Zm-320-320q-19.152 0-32.326-13.174T434.5 256v-80q0-19.152 13.174-32.326T480 130.5q19.152 0 32.326 13.174T525.5 176v80q0 19.152-13.174 32.326T480 301.5Zm0 720q-19.152 0-32.326-13.17Q434.5 995.152 434.5 976v-80q0-19.152 13.174-32.326T480 850.5q19.152 0 32.326 13.174T525.5 896v80q0 19.152-13.174 32.33-13.174 13.17-32.326 13.17ZM222.174 382.065l-43-42Q165.5 327.391 166 308.239t13.174-33.065q13.435-13.674 32.587-13.674t32.065 13.674l42.239 43q12.674 13.435 12.555 31.706-.12 18.272-12.555 31.946-12.674 13.674-31.445 13.413-18.772-.261-32.446-13.174Zm494 494.761-42.239-43q-12.674-13.435-12.674-32.087t12.674-31.565Q686.609 756.5 705.38 757q18.772.5 32.446 13.174l43 41.761Q794.5 824.609 794 843.761t-13.174 33.065Q767.391 890.5 748.239 890.5t-32.065-13.674Zm-42-494.761Q660.5 369.391 661 350.62q.5-18.772 13.174-32.446l41.761-43Q728.609 261.5 747.761 262t33.065 13.174q13.674 13.435 13.674 32.587t-13.674 32.065l-43 42.239q-13.435 12.674-31.706 12.555-18.272-.12-31.946-12.555Zm-495 494.761Q165.5 863.391 165.5 844.239t13.674-32.065l43-42.239q13.435-12.674 32.087-12.674t31.565 12.674Q299.5 782.609 299 801.38q-.5 18.772-13.174 32.446l-41.761 43Q231.391 890.5 212.239 890t-33.065-13.174ZM480 576Z" />
          </svg>
          <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
            class="absolute transition-all scale-0 rotate-90 dark:rotate-0 dark:scale-100">
            <path
              d="M480 936q-151 0-255.5-104.5T120 576q0-138 90-239.5T440 218q25-3 39 18t-1 44q-17 26-25.5 55t-8.5 61q0 90 63 153t153 63q31 0 61.5-9t54.5-25q21-14 43-1.5t19 39.5q-14 138-117.5 229T480 936Zm0-80q88 0 158-48.5T740 681q-20 5-40 8t-40 3q-123 0-209.5-86.5T364 396q0-20 3-40t8-40q-78 32-126.5 102T200 576q0 116 82 198t198 82Zm-10-270Z" />
          </svg>
        </button>
      </nav>
    </div>
  </div>
</header>

    <div class="flex-1"><div class="container flex-1 items-start md:grid md:grid-cols-[220px_minmax(0,1fr)] md:gap-6 lg:grid-cols-[240px_minmax(0,1fr)] lg:gap-10"><aside id="left-sidebar"
  class="fixed inset-y-0 left-0 md:top-14 z-50 md:z-30 bg-background md:bg-transparent transition-all duration-100 -translate-x-full md:translate-x-0 ml-0 p-6 md:p-0 md:-ml-2 md:h-[calc(100vh-3.5rem)] w-5/6 md:w-full shrink-0 overflow-y-auto border-r border-border md:sticky"
  :aria-hidden="!showSidebar" :class="{ 'translate-x-0': showSidebar }">

    <a href="../../../index.html" class="!justify-start text-sm md:!hidden bg-background"><span class="font-bold text-clip whitespace-nowrap">aequitas  documentation</span>
    </a>

    <div class="relative overflow-hidden md:overflow-auto my-4 md:my-0 h-[calc(100vh-8rem)] md:h-auto">
      <div class="overflow-y-auto h-full w-full relative pr-6"><nav class="table w-full min-w-full my-6 lg:my-8">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../30_seconds_aequitas.html">Welcome to Aequitas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../30_seconds_aequitas.html#Bias-measures-tailored-to-your-problem">Bias measures tailored to your problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../input_data.html">Understanding Input Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../output_data.html">Understanding Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../metrics.html">Understanding the Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../CLI.html">Using the CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config.html">Configurations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../using_python.html">Using Aequitas with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../30_seconds_webapp.html">Running the webapp locally</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/index.html">API Docs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/aequitas.html">Aequitas API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/flow.html">Aequitas Flow API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/flow/experiment.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Experiment</span></code> classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/flow/datasets.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Dataset</span></code> classes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html">COMPAS Analysis using Aequitas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html#Pre-Aequitas:-Exploring-the-COMPAS-Dataset">Pre-Aequitas: Exploring the COMPAS Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html#Putting-Aequitas-to-the-task">Putting Aequitas to the task</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html#What-biases-exist-in-my-model?">What biases exist in my model?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html#How-do-I-visualize-bias-in-my-model?">How do I visualize bias in my model?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html#What-levels-of-disparity-exist-between-population-groups?">What levels of disparity exist between population groups?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html#How-do-I-visualize-disparities-in-my-model?">How do I visualize disparities in my model?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html#How-do-I-assess-model-fairness?">How do I assess model fairness?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html#How-do-I-visualize-bias-metric-parity?">How do I visualize bias metric parity?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html#How-do-I-visualize-parity-between-groups-in-my-model?">How do I visualize parity between groups in my model?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/compas_demo.html#The-Aequitas-Effect">The Aequitas Effect</a></li>
</ul>
</li>
</ul>

</nav>
      </div>
    </div>
    <button type="button" @click="showSidebar = false"
      class="absolute md:hidden right-4 top-4 rounded-sm opacity-70 transition-opacity hover:opacity-100">
      <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
        stroke="none" class="h-4 w-4">
        <path
          d="M480 632 284 828q-11 11-28 11t-28-11q-11-11-11-28t11-28l196-196-196-196q-11-11-11-28t11-28q11-11 28-11t28 11l196 196 196-196q11-11 28-11t28 11q11 11 11 28t-11 28L536 576l196 196q11 11 11 28t-11 28q-11 11-28 11t-28-11L480 632Z" />
      </svg>
    </button>
  </aside>
        <main class="relative py-6 lg:gap-10 lg:py-8 xl:grid xl:grid-cols-[1fr_300px]">
<div class="w-full min-w-0 mx-auto">
<nav aria-label="breadcrumbs"
     class="flex items-center mb-4 space-x-1 text-sm text-muted-foreground">
  <a class="overflow-hidden text-ellipsis whitespace-nowrap hover:text-foreground"
     href="../../../index.html">
    <span class="hidden md:inline">aequitas  documentation</span>
    <svg xmlns="http://www.w3.org/2000/svg"
         height="18"
         width="18"
         viewBox="0 96 960 960"
         aria-label="Home"
         fill="currentColor"
         stroke="none"
         class="md:hidden">
      <path d="M240 856h120V616h240v240h120V496L480 316 240 496v360Zm-80 80V456l320-240 320 240v480H520V696h-80v240H160Zm320-350Z" />
    </svg>
  </a>
  
<div class="mr-1">/</div><a class="hover:text-foreground overflow-hidden text-ellipsis whitespace-nowrap"
       href="../../index.html">Module code</a>
    
<div class="mr-1">/</div><span aria-current="page"
        class="font-medium text-foreground overflow-hidden text-ellipsis whitespace-nowrap">src.aequitas.bias</span>
</nav>

    <div id="content" role="main">
      <h1>Source code for src.aequitas.bias</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">aequitas.plotting</span> <span class="kn">import</span> <span class="n">assemble_ref_groups</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">__author__</span> <span class="o">=</span> <span class="s2">&quot;Rayid Ghani, Pedro Saleiro &lt;saleiro@uchicago.edu&gt;, Loren Hinkson&quot;</span>
<span class="n">__copyright__</span> <span class="o">=</span> <span class="s2">&quot;Copyright </span><span class="se">\xa9</span><span class="s2"> 2018. The University of Chicago. All Rights Reserved.&quot;</span>


<div class="viewcode-block" id="Bias">
<a class="viewcode-back" href="../../../api/aequitas.html#src.aequitas.bias.Bias">[docs]</a>
<span class="k">class</span> <span class="nc">Bias</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">default_key_columns</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;model_id&#39;</span><span class="p">,</span> <span class="s1">&#39;score_threshold&#39;</span><span class="p">,</span> <span class="s1">&#39;attribute_name&#39;</span><span class="p">)</span>
    <span class="n">all_group_metrics</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;ppr&#39;</span><span class="p">,</span> <span class="s1">&#39;pprev&#39;</span><span class="p">,</span> <span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="s1">&#39;fdr&#39;</span><span class="p">,</span> <span class="s1">&#39;for&#39;</span><span class="p">,</span> <span class="s1">&#39;fpr&#39;</span><span class="p">,</span>
                         <span class="s1">&#39;fnr&#39;</span><span class="p">,</span> <span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="s1">&#39;tnr&#39;</span><span class="p">,</span> <span class="s1">&#39;npv&#39;</span><span class="p">)</span>
    <span class="n">non_attr_cols</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;score&#39;</span><span class="p">,</span> <span class="s1">&#39;model_id&#39;</span><span class="p">,</span> <span class="s1">&#39;as_of_date&#39;</span><span class="p">,</span> <span class="s1">&#39;entity_id&#39;</span><span class="p">,</span> <span class="s1">&#39;rank_abs&#39;</span><span class="p">,</span>
                     <span class="s1">&#39;rank_pct&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;label_value&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key_columns</span><span class="o">=</span><span class="n">default_key_columns</span><span class="p">,</span> <span class="n">sample_df</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">non_attr_cols</span><span class="o">=</span><span class="n">non_attr_cols</span><span class="p">,</span>
                 <span class="n">input_group_metrics</span><span class="o">=</span><span class="n">all_group_metrics</span><span class="p">,</span> <span class="n">fill_divbyzero</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        :param key_columns: optional, key identifying columns for grouping</span>
<span class="sd">            variables and bias metrics in intermediate joins. Defaults are</span>
<span class="sd">            &#39;model_id&#39;, &#39;score_threshold&#39;, &#39;attribute_name&#39;.</span>
<span class="sd">        :param input_group_metrics: List of absolute bias metrics to calculate</span>
<span class="sd">        :param fill_divbyzero: optional, fill value to use when divided by</span>
<span class="sd">            zero. Default is None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_columns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">key_columns</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_group_metrics</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_group_metrics</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">fill_divbyzero</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fill_divbyzero</span> <span class="o">=</span> <span class="mf">10.00000</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fill_divbyzero</span> <span class="o">=</span> <span class="n">fill_divbyzero</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">non_attr_cols</span> <span class="o">=</span> <span class="n">non_attr_cols</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">significance_cols</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_group_metrics</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">({</span><span class="s1">&#39;label_value&#39;</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">})</span>

<div class="viewcode-block" id="Bias.get_disparity_min_metric">
<a class="viewcode-back" href="../../../api/aequitas.html#src.aequitas.bias.Bias.get_disparity_min_metric">[docs]</a>
    <span class="k">def</span> <span class="nf">get_disparity_min_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">original_df</span><span class="p">,</span> <span class="n">key_columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">input_group_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fill_divbyzero</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">check_significance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">,</span>
                                 <span class="n">mask_significance</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">label_score_ref</span><span class="o">=</span><span class="s1">&#39;fpr&#39;</span><span class="p">,</span>
                                 <span class="n">selected_significance</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates disparities between groups for the predefined list of</span>
<span class="sd">        group metrics using the group with the minimum value for each absolute</span>
<span class="sd">        bias metric as the reference group (denominator).</span>

<span class="sd">        :param df: output dataframe of Group class get_crosstabs() method.</span>
<span class="sd">        :param original_df: a dataframe of sample features and model results.</span>
<span class="sd">            Includes a required &#39;score &#39;column and possible &#39;label_value&#39; column.</span>
<span class="sd">        :param key_columns: optional, key identifying columns for grouping</span>
<span class="sd">            variables and bias metrics in intermediate joins. Defaults are</span>
<span class="sd">            &#39;model_id&#39;, &#39;score_threshold&#39;, &#39;attribute_name&#39;.</span>
<span class="sd">        :param input_group_metrics: optional, columns list corresponding to</span>
<span class="sd">            the group metrics for which we want to calculate disparity values</span>
<span class="sd">        :param fill_divbyzero: optional, fill value to use when divided by</span>
<span class="sd">            zero. Default is None.</span>
<span class="sd">        :param check_significance: whether to determine statistical signifance</span>
<span class="sd">            for disparity metrics. Default is False.</span>
<span class="sd">        :param selected_significance: specific measures (beyond label_value and</span>
<span class="sd">            score) to which to limit statistical significance calculations when</span>
<span class="sd">            check_significance is True. Default is False, i.e., significance</span>
<span class="sd">            will be calculated for all metrics.</span>
<span class="sd">        :param alpha: statistical significance level to use in significance</span>
<span class="sd">            determination. Default is 5e-2 (0.05).</span>
<span class="sd">        :param mask_significance: whether to display a T/F mask over calculated</span>
<span class="sd">            p-values from statistical significance determination. Default is True.</span>
<span class="sd">        :param label_score_ref: default reference group to use for score and</span>
<span class="sd">            label_value statistical significance calculations.</span>
<span class="sd">        :return: A dataframe with same number of rows as the input (crosstab)</span>
<span class="sd">            with additional disparity metrics columns and ref_group_values</span>
<span class="sd">            for each metric.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># record df column order</span>
        <span class="n">original_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">key_columns</span><span class="p">:</span>
            <span class="n">key_columns</span> <span class="o">=</span> <span class="n">original_cols</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_columns</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">input_group_metrics</span><span class="p">:</span>
            <span class="n">input_group_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_group_metrics</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">fill_divbyzero</span><span class="p">:</span>
            <span class="n">fill_divbyzero</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fill_divbyzero</span>

        <span class="k">for</span> <span class="n">group_metric</span> <span class="ow">in</span> <span class="n">input_group_metrics</span><span class="p">:</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># this groupby is being called every cycle. maybe we can create</span>
                <span class="c1"># a list of df_groups and merge df at the end? it can not be</span>
                <span class="c1"># simply put outside the loop(the merge...)</span>
                <span class="n">idxmin</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">key_columns</span><span class="p">)[</span><span class="n">group_metric</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()</span>

                <span class="c1"># if entire column for a group metric is NaN, cast min value index</span>
                <span class="c1"># column to the same index as that of any other group for that attribute</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">idxmin</span><span class="o">.</span><span class="n">values</span><span class="p">):</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">idxmin</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">idxmin_not_nan</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">key_columns</span><span class="p">)[</span><span class="s1">&#39;index&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
                        <span class="n">idxmin</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idxmin</span><span class="o">.</span><span class="n">isna</span><span class="p">()]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span>
                            <span class="n">left</span><span class="o">=</span><span class="n">idxmin</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idxmin</span><span class="o">.</span><span class="n">isna</span><span class="p">()],</span> <span class="n">right</span><span class="o">=</span><span class="n">idxmin_not_nan</span><span class="p">,</span>
                            <span class="n">left_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;inner&#39;</span><span class="p">,</span>
                        <span class="p">)[</span><span class="s1">&#39;index&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;A minimum value for group_metric &quot;</span>
                                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">group_metric</span><span class="si">}</span><span class="s2"> could not be calculated.&quot;</span><span class="p">)</span>

                <span class="n">df_min_idx</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idxmin</span><span class="p">]</span>

                <span class="c1"># but we also want to get the group_value of the reference group</span>
                <span class="c1"># for each bias metric</span>
                <span class="n">df_to_merge</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
                <span class="n">df_to_merge</span><span class="p">[</span><span class="n">key_columns</span> <span class="o">+</span> <span class="p">[</span><span class="n">group_metric</span> <span class="o">+</span> <span class="s1">&#39;_disparity&#39;</span><span class="p">,</span> <span class="n">group_metric</span> <span class="o">+</span>
                                           <span class="s1">&#39;_ref_group_value&#39;</span><span class="p">]]</span> <span class="o">=</span> \
                    <span class="n">df_min_idx</span><span class="p">[</span><span class="n">key_columns</span> <span class="o">+</span> <span class="p">[</span><span class="n">group_metric</span><span class="p">,</span> <span class="s1">&#39;attribute_value&#39;</span><span class="p">]]</span>

            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                    <span class="s1">&#39;get_bias_min_metric:: one of the following columns is not &#39;</span>
                    <span class="s1">&#39;on the input dataframe : model_id, parameter, attribute_name &#39;</span>
                    <span class="s1">&#39;or any of the input_group_metrics &#39;</span>
                    <span class="s1">&#39;list&#39;</span><span class="p">)</span>

            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df_to_merge</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="n">key_columns</span><span class="p">)</span>
            <span class="c1"># creating disparity by dividing each group metric value by the</span>
            <span class="c1"># corresponding min value from the groups of the target attribute</span>
            <span class="n">df</span><span class="p">[</span><span class="n">group_metric</span> <span class="o">+</span> <span class="s1">&#39;_disparity&#39;</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">df</span><span class="p">[</span><span class="n">group_metric</span><span class="p">]</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="n">group_metric</span> <span class="o">+</span> <span class="s1">&#39;_disparity&#39;</span><span class="p">]</span>
            <span class="c1"># We are capping the disparity values to 10.0 when divided by zero...</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">fill_divbyzero</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">check_significance</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">df</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># add statistical_significance</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">selected_significance</span><span class="p">:</span>
                <span class="n">selected_significance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_group_metrics</span>

                <span class="c1"># only proceed with columns actually in dataframe</span>
            <span class="n">selected_significance</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">original_cols</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">selected_significance</span><span class="p">))</span>

            <span class="c1"># always includes label and score significance</span>
            <span class="n">selected_significance</span> <span class="o">=</span> <span class="n">selected_significance</span><span class="o">.</span><span class="n">union</span><span class="p">({</span><span class="s1">&#39;label_value&#39;</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">})</span>

            <span class="n">ref_groups_dict</span> <span class="o">=</span> <span class="n">assemble_ref_groups</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">ref_group_flag</span><span class="o">=</span><span class="s1">&#39;_ref_group_value&#39;</span><span class="p">,</span>
                                                  <span class="n">specific_measures</span><span class="o">=</span><span class="n">selected_significance</span><span class="p">,</span>
                                                  <span class="n">label_score_ref</span><span class="o">=</span><span class="n">label_score_ref</span><span class="p">)</span>

            <span class="n">attr_cols</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;attribute_name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
            <span class="c1"># run significance method on bias-augmented crosstab based on false</span>
            <span class="c1"># positives, false negatives, scores, and label values in original df</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_statistical_significance</span><span class="p">(</span>
                <span class="n">original_df</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ref_dict</span><span class="o">=</span><span class="n">ref_groups_dict</span><span class="p">,</span> <span class="n">score_thresholds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">attr_cols</span><span class="o">=</span><span class="n">attr_cols</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">,</span> <span class="n">selected_significance</span><span class="o">=</span><span class="n">selected_significance</span><span class="p">)</span>

            <span class="c1"># if specified, apply T/F mask to significance columns</span>
            <span class="k">if</span> <span class="n">mask_significance</span><span class="p">:</span>
                <span class="n">significance_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;_significance&#39;</span><span class="p">)]</span>
                <span class="n">truemask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">significance_cols</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">alpha</span>
                <span class="n">falsemask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">significance_cols</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">alpha</span>

                <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">significance_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">truemask</span><span class="p">,</span> <span class="n">falsemask</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

            <span class="c1"># order new calculated metric columns: disparity, ref_group, then</span>
            <span class="c1"># significance for each</span>
            <span class="n">base_sig</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;label_value_significance&#39;</span><span class="p">,</span> <span class="s1">&#39;score_significance&#39;</span><span class="p">]</span>

            <span class="n">new_cols</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">original_cols</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">base_sig</span><span class="p">))</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="n">original_cols</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="n">base_sig</span> <span class="o">+</span> <span class="n">new_cols</span><span class="p">]</span></div>



<div class="viewcode-block" id="Bias.get_disparity_major_group">
<a class="viewcode-back" href="../../../api/aequitas.html#src.aequitas.bias.Bias.get_disparity_major_group">[docs]</a>
    <span class="k">def</span> <span class="nf">get_disparity_major_group</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">original_df</span><span class="p">,</span> <span class="n">key_columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">input_group_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">fill_divbyzero</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">check_significance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">alpha</span> <span class="o">=</span> <span class="mf">5e-2</span><span class="p">,</span> <span class="n">mask_significance</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                  <span class="n">selected_significance</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates disparities between groups for the predefined list of group</span>
<span class="sd">        metrics using the majority group within each attribute as the reference</span>
<span class="sd">        group (denominator).</span>

<span class="sd">        :param df: output dataframe of Group class get_crosstabs() method.</span>
<span class="sd">        :param original_df: a dataframe of sample features and model results.</span>
<span class="sd">            Includes a required &#39;score &#39;column and possible &#39;label_value&#39; column.</span>
<span class="sd">        :param key_columns: optional, key identifying columns for grouping</span>
<span class="sd">            variables and bias metrics in intermediate joins. Defaults are</span>
<span class="sd">            &#39;model_id&#39;, &#39;score_threshold&#39;, &#39;attribute_name&#39;.</span>
<span class="sd">        :param input_group_metrics: optional, columns list corresponding to</span>
<span class="sd">            the group metrics for which we want to calculate disparity values</span>
<span class="sd">        :param fill_divbyzero: optional, fill value to use when divided by</span>
<span class="sd">            zero. Default is None.</span>
<span class="sd">        :param check_significance: whether to determine statistical signifance</span>
<span class="sd">            for disparity metrics. Default is False.</span>
<span class="sd">        :param selected_significance: specific measures (beyond label_value and</span>
<span class="sd">            score) to which to limit statistical significance calculations when</span>
<span class="sd">            check_significance is True. Default is False, i.e., significance</span>
<span class="sd">            will be calculated for all metrics.</span>
<span class="sd">        :param alpha: statistical significance level to use in significance</span>
<span class="sd">            determination. Default is 5e-2 (0.05).</span>
<span class="sd">        :param mask_significance: whether to display a T/F mask over calculated</span>
<span class="sd">            p-values from statistical significance determination. Default is True.</span>
<span class="sd">        :return: A dataframe with same number of rows as the input (crosstab)</span>
<span class="sd">            with additional disparity metrics columns and ref_group_values</span>
<span class="sd">            for each metric.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># record df column order</span>
        <span class="n">original_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">key_columns</span><span class="p">:</span>
            <span class="n">key_columns</span> <span class="o">=</span> <span class="n">original_cols</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_columns</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">input_group_metrics</span><span class="p">:</span>
            <span class="n">input_group_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_group_metrics</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">fill_divbyzero</span><span class="p">:</span>
            <span class="n">fill_divbyzero</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fill_divbyzero</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">df_major_group</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">key_columns</span><span class="p">)[</span><span class="s1">&#39;group_size&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s1">&#39;get_bias_major_group:: one of the following columns &#39;</span>
                          <span class="s1">&#39;is not on the input dataframe : model_id, parameter, &#39;</span>
                          <span class="s1">&#39;attribute_name, group_size&#39;</span><span class="p">)</span>

        <span class="n">disparity_metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="o">+</span> <span class="s1">&#39;_disparity&#39;</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">input_group_metrics</span><span class="p">]</span>
        <span class="n">df_to_merge</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

        <span class="c1"># we created the df_to_merge has a subset of the df_ref_group containing</span>
        <span class="c1"># the target ref group values which are now labeled as _disparity but</span>
        <span class="c1"># we still need to perform the division</span>
        <span class="n">df_to_merge</span><span class="p">[</span><span class="n">key_columns</span> <span class="o">+</span> <span class="n">disparity_metrics</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_major_group</span><span class="p">[</span>
            <span class="n">key_columns</span> <span class="o">+</span> <span class="n">input_group_metrics</span><span class="p">]</span>

        <span class="c1"># we now need to create the ref_group_value columns in the df_to_merge</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">input_group_metrics</span><span class="p">:</span>
            <span class="n">df_to_merge</span><span class="p">[</span><span class="n">col</span> <span class="o">+</span> <span class="s1">&#39;_ref_group_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_major_group</span><span class="p">[</span><span class="s1">&#39;attribute_value&#39;</span><span class="p">]</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df_to_merge</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="n">key_columns</span><span class="p">)</span>
        <span class="n">df</span><span class="p">[</span><span class="n">disparity_metrics</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">input_group_metrics</span><span class="p">]</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">disparity_metrics</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

        <span class="c1"># We are capping the disparity values to 10.0 when divided by zero...</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">fill_divbyzero</span><span class="p">)</span>

        <span class="c1"># when there is a zero in the numerator and a zero in denominator it is</span>
        <span class="c1"># considered NaN after division, so if 0/0 we assume 1.0 disparity</span>
        <span class="c1"># (they are the same...)</span>

        <span class="c1"># default is to use the same ref groups as df, need to add functionality to</span>
        <span class="c1"># compile ref_groups_dict based on a passed ref group for a given measure</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">check_significance</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">df</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">selected_significance</span><span class="p">:</span>
                <span class="n">selected_significance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_group_metrics</span>

            <span class="c1"># only proceed with columns actually in dataframe</span>
            <span class="n">selected_significance</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">original_cols</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">selected_significance</span><span class="p">))</span>

            <span class="c1"># always includes label and score significance</span>
            <span class="n">selected_significance</span> <span class="o">=</span> <span class="n">selected_significance</span><span class="o">.</span><span class="n">union</span><span class="p">({</span><span class="s1">&#39;label_value&#39;</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">})</span>

            <span class="n">ref_groups_dict</span> <span class="o">=</span> <span class="n">assemble_ref_groups</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">ref_group_flag</span><span class="o">=</span><span class="s1">&#39;_ref_group_value&#39;</span><span class="p">,</span>
                                                  <span class="n">specific_measures</span><span class="o">=</span><span class="n">selected_significance</span><span class="p">,</span>
                                                  <span class="n">label_score_ref</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

            <span class="n">attr_cols</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;attribute_name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">attribute</span> <span class="ow">in</span> <span class="n">attr_cols</span><span class="p">:</span>
                <span class="n">largest_group</span> <span class="o">=</span> <span class="n">df_major_group</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_major_group</span><span class="p">[</span><span class="s1">&#39;attribute_name&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">attribute</span><span class="p">,</span>
                                                   <span class="s1">&#39;attribute_value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">ref_groups_dict</span><span class="p">[</span><span class="n">attribute</span><span class="p">][</span><span class="s1">&#39;label_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">largest_group</span>
                <span class="n">ref_groups_dict</span><span class="p">[</span><span class="n">attribute</span><span class="p">][</span><span class="s1">&#39;score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">largest_group</span>


            <span class="c1"># run significance method on bias-augmented crosstab based on false</span>
            <span class="c1"># positives, false negatives, scores, and label values in original df</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_statistical_significance</span><span class="p">(</span>
                <span class="n">original_df</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ref_dict</span><span class="o">=</span><span class="n">ref_groups_dict</span><span class="p">,</span> <span class="n">score_thresholds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">attr_cols</span><span class="o">=</span><span class="n">attr_cols</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">,</span> <span class="n">selected_significance</span><span class="o">=</span><span class="n">selected_significance</span><span class="p">)</span>

            <span class="c1"># if specified, apply T/F mask to significance columns</span>
            <span class="k">if</span> <span class="n">mask_significance</span><span class="p">:</span>
                <span class="n">significance_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;_significance&#39;</span><span class="p">)]</span>
                <span class="n">truemask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">significance_cols</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">alpha</span>
                <span class="n">falsemask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">significance_cols</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">alpha</span>

                <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">significance_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">truemask</span><span class="p">,</span> <span class="n">falsemask</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

            <span class="c1"># check what new disparity columns are and order as disparity,</span>
            <span class="c1"># ref_group, significance for each</span>
            <span class="n">base_sig</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;label_value_significance&#39;</span><span class="p">,</span> <span class="s1">&#39;score_significance&#39;</span><span class="p">]</span>

            <span class="n">new_cols</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">original_cols</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">base_sig</span><span class="p">))</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="n">original_cols</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="n">base_sig</span> <span class="o">+</span> <span class="n">new_cols</span><span class="p">]</span></div>



    <span class="k">def</span> <span class="nf">_verify_ref_groups_dict_len</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ref_groups_dict</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ref_groups_dict</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;attribute_name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>

    <span class="k">def</span> <span class="nf">_verify_ref_group_loc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">group_slice</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">group_slice</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>

<div class="viewcode-block" id="Bias.get_disparity_predefined_groups">
<a class="viewcode-back" href="../../../api/aequitas.html#src.aequitas.bias.Bias.get_disparity_predefined_groups">[docs]</a>
    <span class="k">def</span> <span class="nf">get_disparity_predefined_groups</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">original_df</span><span class="p">,</span> <span class="n">ref_groups_dict</span><span class="p">,</span>
                                        <span class="n">key_columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                        <span class="n">input_group_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                        <span class="n">fill_divbyzero</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                        <span class="n">check_significance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">,</span>
                                        <span class="n">mask_significance</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                        <span class="n">selected_significance</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates disparities between groups for the predefined list of group</span>
<span class="sd">        metrics using a predefined reference group (denominator) value for each</span>
<span class="sd">        attribute.</span>

<span class="sd">        :param df: output dataframe of Group class get_crosstabs() method.</span>
<span class="sd">        :param original_df: dataframe of sample features and model results.</span>
<span class="sd">            Includes a required &#39;score &#39;column and possible &#39;label_value&#39; column.</span>
<span class="sd">        :param ref_groups_dict: dictionary of format: {&#39;attribute_name&#39;: &#39;attribute_value&#39;, ...}</span>
<span class="sd">        :param key_columns: optional, key identifying columns for grouping</span>
<span class="sd">            variables and bias metrics in intermediate joins. Defaults are</span>
<span class="sd">            &#39;model_id&#39;, &#39;score_threshold&#39;, &#39;attribute_name&#39;.</span>
<span class="sd">        :param input_group_metrics: optional, columns list corresponding to</span>
<span class="sd">            the group metrics for which we want to calculate disparity values</span>
<span class="sd">        :param fill_divbyzero: optional, fill value to use when divided by</span>
<span class="sd">            zero. Default is None.</span>
<span class="sd">        :param check_significance: whether to determine statistical signifance</span>
<span class="sd">            for disparity metrics. Default is False.</span>
<span class="sd">        :param selected_significance: specific measures (beyond label_value and</span>
<span class="sd">            score) to which to limit statistical significance calculations when</span>
<span class="sd">            check_significance is True. Default is False, i.e., significance</span>
<span class="sd">            will be calculated for all metrics.</span>
<span class="sd">        :param alpha: statistical significance level to use in significance</span>
<span class="sd">            determination. Default is 5e-2 (0.05).</span>
<span class="sd">        :param mask_significance: whether to display a T/F mask over calculated</span>
<span class="sd">            p-values from statistical significance determination. Default is True.</span>
<span class="sd">        :return: A dataframe with same number of rows as the input (crosstab)</span>
<span class="sd">            with additional disparity metrics columns and ref_group_values</span>
<span class="sd">            for each metric.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># record df column order</span>
        <span class="n">original_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">key_columns</span><span class="p">:</span>
            <span class="n">key_columns</span> <span class="o">=</span> <span class="n">original_cols</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_columns</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">input_group_metrics</span><span class="p">:</span>
            <span class="n">input_group_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_group_metrics</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">fill_divbyzero</span><span class="p">:</span>
            <span class="n">fill_divbyzero</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fill_divbyzero</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_verify_ref_groups_dict_len</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">ref_groups_dict</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Bias.get_disparity_predefined_groups(): the number of &#39;</span>
                          <span class="s1">&#39;predefined group values to use as reference is less &#39;</span>
                          <span class="s1">&#39;than the actual number of attributes in the input &#39;</span>
                          <span class="s1">&#39;dataframe.&#39;</span><span class="p">)</span>

        <span class="n">df_ref_group</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">ref_groups_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">group_slice</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;attribute_name&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">key</span><span class="p">)</span> <span class="o">&amp;</span>
                                     <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;attribute_value&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">val</span><span class="p">)]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_verify_ref_group_loc</span><span class="p">(</span><span class="n">group_slice</span><span class="p">)</span>
                <span class="n">df_ref_group</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_ref_group</span><span class="p">,</span> <span class="n">group_slice</span><span class="p">])</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s1">&#39;get_disparity_predefined_groups(): reference groups &#39;</span>
                          <span class="s1">&#39;and values provided do not exist as columns/values &#39;</span>
                          <span class="s1">&#39;in the input dataframe.(Note: check for syntax errors)&#39;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;get_disparity_predefined_groups(): reference groups &#39;</span>
                           <span class="s1">&#39;and values provided do not exist as columns/values &#39;</span>
                           <span class="s1">&#39;in the input dataframe.(Note: check for syntax errors)&#39;</span><span class="p">)</span>

        <span class="n">disparity_metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="o">+</span> <span class="s1">&#39;_disparity&#39;</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">input_group_metrics</span><span class="p">]</span>
        <span class="n">df_to_merge</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

        <span class="c1"># we created the df_to_merge has a subset of the df_ref_group containing</span>
        <span class="c1"># the target ref group values which are now labeled as _disparity but</span>
        <span class="c1"># we still need to perform the division</span>
        <span class="n">df_to_merge</span><span class="p">[</span><span class="n">key_columns</span> <span class="o">+</span> <span class="n">disparity_metrics</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_ref_group</span><span class="p">[</span>
            <span class="n">key_columns</span> <span class="o">+</span> <span class="n">input_group_metrics</span><span class="p">]</span>

        <span class="c1"># we now need to create the ref_group_value columns in the df_to_merge</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">input_group_metrics</span><span class="p">:</span>
            <span class="n">df_to_merge</span><span class="p">[</span><span class="n">col</span> <span class="o">+</span> <span class="s1">&#39;_ref_group_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_ref_group</span><span class="p">[</span><span class="s1">&#39;attribute_value&#39;</span><span class="p">]</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df_to_merge</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="n">key_columns</span><span class="p">)</span>
        <span class="n">df</span><span class="p">[</span><span class="n">disparity_metrics</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">input_group_metrics</span><span class="p">]</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">disparity_metrics</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

        <span class="c1"># We are capping the disparity values to 10.0 when divided by zero...</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">fill_divbyzero</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">check_significance</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">df</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">selected_significance</span><span class="p">:</span>
                <span class="n">selected_significance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_group_metrics</span>

            <span class="c1"># only proceed with columns actually in dataframe</span>
            <span class="n">selected_significance</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span> <span class="n">original_cols</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">selected_significance</span><span class="p">)</span> <span class="p">)</span>

            <span class="c1"># always includes label and score significance</span>
            <span class="n">selected_significance</span> <span class="o">=</span> <span class="n">selected_significance</span><span class="o">.</span><span class="n">union</span><span class="p">({</span><span class="s1">&#39;label_value&#39;</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">})</span>

            <span class="c1"># compile dictionary of reference groups based on bias-augmented crosstab</span>
            <span class="n">full_ref_dict</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="c1"># for predefined groups, use the largest of the predefined groups as</span>
            <span class="c1"># ref group for score and label value</span>
            <span class="c1"># key is an attribute_name, val is an attribute_value</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">ref_groups_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">full_ref_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;label_value&#39;</span><span class="p">:</span> <span class="n">val</span><span class="p">,</span>
                                      <span class="s1">&#39;score&#39;</span><span class="p">:</span> <span class="n">val</span><span class="p">}</span>
                <span class="k">for</span> <span class="n">measure</span> <span class="ow">in</span> <span class="n">selected_significance</span><span class="p">:</span>
                    <span class="n">full_ref_dict</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">measure</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>

            <span class="c1"># run significance method on bias-augmented crosstab based on false</span>
            <span class="c1"># positives, false negatives, scores, and label values in original df</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_statistical_significance</span><span class="p">(</span>
                <span class="n">original_df</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ref_dict</span><span class="o">=</span><span class="n">full_ref_dict</span><span class="p">,</span> <span class="n">score_thresholds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">attr_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">,</span> <span class="n">selected_significance</span><span class="o">=</span><span class="n">selected_significance</span><span class="p">)</span>

            <span class="c1"># if specified, apply T/F mask to significance columns</span>
            <span class="k">if</span> <span class="n">mask_significance</span><span class="p">:</span>
                <span class="n">significance_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;_significance&#39;</span><span class="p">)]</span>
                <span class="n">truemask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">significance_cols</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">alpha</span>
                <span class="n">falsemask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">significance_cols</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">alpha</span>

                <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">significance_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">truemask</span><span class="p">,</span> <span class="n">falsemask</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

            <span class="c1"># check what new disparity columns are and order as disparity,</span>
            <span class="c1"># ref_group, significance for each</span>
            <span class="n">base_sig</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;label_value_significance&#39;</span><span class="p">,</span> <span class="s1">&#39;score_significance&#39;</span><span class="p">]</span>

            <span class="n">new_cols</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">original_cols</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">base_sig</span><span class="p">))</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="n">original_cols</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="n">base_sig</span> <span class="o">+</span> <span class="n">new_cols</span><span class="p">]</span></div>


    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_measure_sample</span><span class="p">(</span><span class="n">original_df</span><span class="p">,</span> <span class="n">attribute</span><span class="p">,</span> <span class="n">measure</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function for _get_statistical_significance() (via</span>
<span class="sd">        _calculate_significance() function). Convert dataframe to samples for</span>
<span class="sd">        given attribute group.</span>

<span class="sd">        :param original_df: a dataframe containing a required raw &#39;score&#39; column</span>
<span class="sd">            and possible raw &#39;label_value&#39; column.</span>
<span class="sd">        :param attribute: Attribute of interest in dataset (ex: race, sex, age</span>
<span class="sd">            category)</span>
<span class="sd">        :param measure: Metric of interest for which to calculate significance</span>
<span class="sd">            (false positives, false negatives, score, label_value)</span>

<span class="sd">        :return: A dictionary of binary &#39;samples&#39; for each attribute group</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">original_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">attribute</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">f</span><span class="p">:</span> <span class="n">f</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">f</span><span class="p">[</span><span class="n">measure</span><span class="p">]</span><span class="o">.</span><span class="n">notnull</span><span class="p">(),</span> <span class="n">measure</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_check_equal_variance</span><span class="p">(</span><span class="n">sample_dict</span><span class="p">,</span> <span class="n">ref_group</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function for _get_statistical_significance() (via</span>
<span class="sd">        _calculate_significance() function).</span>

<span class="sd">        :param sample_dict: dictionary of binary samples for equal variance</span>
<span class="sd">            comparison.</span>
<span class="sd">        :param ref_group: Group to use as reference group for statistical</span>
<span class="sd">            significance calculation.</span>
<span class="sd">        :param alpha: Level at which to determine statistical significance.</span>
<span class="sd">            Default is 5e-2 (0.05).</span>

<span class="sd">        :return: Dictionary indicating whether each group has equal variance</span>
<span class="sd">        (in comparison with reference group)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Immediately set ref_group status for equal variance (with itself): True</span>

        <span class="n">eq_variance</span> <span class="o">=</span> <span class="p">{</span><span class="n">ref_group</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>

        <span class="k">for</span> <span class="n">attr_value</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">sample_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># make default normality_p value (only used when len(sample) &lt; 20)</span>
            <span class="c1"># large enough that it is always greater than alpha</span>
            <span class="n">normality_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

            <span class="c1"># skew test requires at least 20 samples</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">:</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">normality_p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">normaltest</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nan_policy</span><span class="o">=</span><span class="s1">&#39;omit&#39;</span><span class="p">)</span>

            <span class="c1"># if tested normality is False or less than 8 samples, use levene</span>
            <span class="c1"># test to check equal variance between groups</span>
            <span class="k">if</span> <span class="n">normality_p</span> <span class="o">&lt;</span> <span class="n">alpha</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">20</span><span class="p">:</span>
                <span class="c1"># if ref_group is not normal, can&#39;t use f-test or bartlett test</span>
                <span class="c1"># for any samples, so check for equal variance against ref_group</span>
                <span class="c1"># using levene test for all groups and return dict</span>
                <span class="c1"># (since includes all groups)</span>
                <span class="k">if</span> <span class="n">attr_value</span> <span class="o">==</span> <span class="n">ref_group</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">group</span><span class="p">,</span> <span class="n">sample_list</span> <span class="ow">in</span> <span class="n">sample_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="n">_</span><span class="p">,</span> <span class="n">equal_variance_p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">levene</span><span class="p">(</span><span class="n">sample_dict</span><span class="p">[</span><span class="n">ref_group</span><span class="p">],</span>
                                                           <span class="n">sample_list</span><span class="p">,</span>
                                                           <span class="n">center</span><span class="o">=</span><span class="s1">&#39;median&#39;</span><span class="p">)</span>

                        <span class="n">eq_variance</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">=</span> <span class="n">equal_variance_p</span> <span class="o">&gt;=</span> <span class="n">alpha</span>

                    <span class="k">return</span> <span class="n">eq_variance</span>

                <span class="c1"># if a non-ref group is not normal, can&#39;t use f-test or bartlett</span>
                <span class="c1"># for that group, check for equal variance (against ref_group)</span>
                <span class="c1"># using levene test and add result to dictionary</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">equal_variance_p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">levene</span><span class="p">(</span>
                    <span class="n">sample_dict</span><span class="p">[</span><span class="n">ref_group</span><span class="p">],</span> <span class="n">sample</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="s1">&#39;median&#39;</span><span class="p">)</span>

                <span class="n">eq_variance</span><span class="p">[</span><span class="n">attr_value</span><span class="p">]</span> <span class="o">=</span> <span class="n">equal_variance_p</span> <span class="o">&gt;=</span> <span class="n">alpha</span>

        <span class="c1"># for all normally distributed non-ref groups, use bartlett test to</span>
        <span class="c1"># check for equal variance (against ref_group). Add results to dict</span>
        <span class="n">untested_groups</span> <span class="o">=</span> <span class="n">sample_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="o">-</span> <span class="n">eq_variance</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">ref_group</span><span class="p">)</span>
        <span class="n">untested</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">val</span> <span class="k">for</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="ow">in</span> <span class="n">sample_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">untested_groups</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">attr_value</span><span class="p">,</span> <span class="n">sample_list</span> <span class="ow">in</span> <span class="n">untested</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">equal_variance_p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">bartlett</span><span class="p">(</span><span class="n">sample_dict</span><span class="p">[</span><span class="n">ref_group</span><span class="p">],</span> <span class="n">sample_list</span><span class="p">)</span>

            <span class="n">eq_variance</span><span class="p">[</span><span class="n">attr_value</span><span class="p">]</span> <span class="o">=</span> <span class="n">equal_variance_p</span> <span class="o">&gt;=</span> <span class="n">alpha</span>

        <span class="k">return</span> <span class="n">eq_variance</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_calculate_significance</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">original_df</span><span class="p">,</span> <span class="n">disparity_df</span><span class="p">,</span> <span class="n">attribute</span><span class="p">,</span>
                                <span class="n">measure</span><span class="p">,</span> <span class="n">ref_dict</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function for _get_statistical_significance. Pulls samples from</span>
<span class="sd">        original df, checks for equal variance between population groups and</span>
<span class="sd">        reference group, runs t-test between groups and reference group, and</span>
<span class="sd">        adds p-values to disparity_df for a given measure (ex: false positives)</span>

<span class="sd">        :param original_df: a dataframe containing a required raw &#39;score&#39; column</span>
<span class="sd">            and possible raw &#39;label_value&#39; column.</span>
<span class="sd">        :param disparity_df: Expansion of get_crosstabs() output with additional</span>
<span class="sd">            columns for disparity metrics and each metric&#39;s reference group to</span>
<span class="sd">            which significance must be added</span>
<span class="sd">        :param attribute: Attribute for which to calculate statistical</span>
<span class="sd">            significance of a measure</span>
<span class="sd">        :param measure: Measure for which to calculate statistical significance</span>
<span class="sd">        :param ref_dict: Dictionary indicating reference group for each</span>
<span class="sd">            attribute/ measure combination</span>
<span class="sd">        :param alpha: Level at which to determine statistical significance.</span>
<span class="sd">            Default is 5e-2 (0.05).</span>

<span class="sd">        :return: dataframe with same number of rows as the input (crosstab)</span>
<span class="sd">            but with additional disparity metrics columns, ref_group_values, and</span>
<span class="sd">            statistical significance (p-values) of specific metrics.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">binaries_lookup</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;label_value&#39;</span><span class="p">:</span> <span class="s1">&#39;label_value&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_fpr&#39;</span><span class="p">:</span> <span class="s1">&#39;fpr&#39;</span><span class="p">,</span>
                           <span class="s1">&#39;binary_tpr&#39;</span><span class="p">:</span> <span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_tnr&#39;</span><span class="p">:</span> <span class="s1">&#39;tnr&#39;</span><span class="p">,</span>
                           <span class="s1">&#39;binary_fnr&#39;</span><span class="p">:</span> <span class="s1">&#39;fnr&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_score&#39;</span><span class="p">:</span> <span class="s1">&#39;score&#39;</span><span class="p">,</span>
                           <span class="s1">&#39;binary_precision&#39;</span><span class="p">:</span> <span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_npv&#39;</span><span class="p">:</span> <span class="s1">&#39;npv&#39;</span><span class="p">,</span>
                           <span class="s1">&#39;binary_for&#39;</span><span class="p">:</span> <span class="s1">&#39;for&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_fdr&#39;</span><span class="p">:</span> <span class="s1">&#39;fdr&#39;</span><span class="p">,</span>
                           <span class="s1">&#39;binary_ppr&#39;</span><span class="p">:</span> <span class="s1">&#39;ppr&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_pprev&#39;</span><span class="p">:</span> <span class="s1">&#39;pprev&#39;</span><span class="p">}</span>

        <span class="n">ref_group</span> <span class="o">=</span> <span class="n">ref_dict</span><span class="p">[</span><span class="n">attribute</span><span class="p">][</span><span class="n">binaries_lookup</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">measure</span><span class="p">)]</span>

        <span class="c1"># create dictionary of &quot;samples&quot; (binary values for false positive,</span>
        <span class="c1"># false negative, label value, score) based on original data frame</span>
        <span class="n">sample_dict</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_get_measure_sample</span><span class="p">(</span><span class="n">original_df</span><span class="o">=</span><span class="n">original_df</span><span class="p">,</span>
                                              <span class="n">attribute</span><span class="o">=</span><span class="n">attribute</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">measure</span><span class="p">)</span>

        <span class="c1"># run SciPy equal variance tests between each group and a given</span>
        <span class="c1"># reference group, store results in dictionary to pass to statistical</span>
        <span class="c1"># significance tests</span>
        <span class="n">eq_variance_dict</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_check_equal_variance</span><span class="p">(</span><span class="n">sample_dict</span><span class="o">=</span><span class="n">sample_dict</span><span class="p">,</span>
                                                     <span class="n">ref_group</span><span class="o">=</span><span class="n">ref_group</span><span class="p">,</span>
                                                     <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        <span class="c1"># run SciPy statistical significance test between each group and</span>
        <span class="c1"># reference group</span>
        <span class="k">for</span> <span class="n">attr_val</span><span class="p">,</span> <span class="n">eq_var</span> <span class="ow">in</span> <span class="n">sample_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>

            <span class="n">_</span><span class="p">,</span> <span class="n">difference_significance_p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">ref_group</span><span class="p">],</span>
                <span class="n">sample_dict</span><span class="p">[</span><span class="n">attr_val</span><span class="p">],</span>
                <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">equal_var</span><span class="o">=</span><span class="n">eq_variance_dict</span><span class="p">[</span><span class="n">attr_val</span><span class="p">],</span>
                <span class="n">nan_policy</span><span class="o">=</span><span class="s1">&#39;omit&#39;</span><span class="p">)</span>

            <span class="n">measure</span> <span class="o">=</span> <span class="n">measure</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;binary_&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>

            <span class="c1"># add column to crosstab to indicate statistical significance</span>
            <span class="n">disparity_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">disparity_df</span><span class="p">[</span><span class="s1">&#39;attribute_value&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">attr_val</span><span class="p">,</span>
                             <span class="n">measure</span> <span class="o">+</span> <span class="s1">&#39;_significance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">difference_significance_p</span>

        <span class="k">return</span> <span class="n">disparity_df</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_get_statistical_significance</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">original_df</span><span class="p">,</span> <span class="n">disparity_df</span><span class="p">,</span> <span class="n">ref_dict</span><span class="p">,</span>
                                      <span class="n">score_thresholds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">attr_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">,</span>
                                      <span class="n">selected_significance</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        :param original_df: a dataframe containing a required raw &#39;score&#39; column</span>
<span class="sd">            and possible raw &#39;label_value&#39; column.</span>
<span class="sd">        :param disparity_df: Expansion of get_crosstabs() output with additional</span>
<span class="sd">            columns for disparity metrics and each metric&#39;s reference group to</span>
<span class="sd">            which significance must be added</span>
<span class="sd">        :param ref_dict: Dictionary indicating reference group for each</span>
<span class="sd">            attribute/ measure combination</span>
<span class="sd">        :param score_thresholds: a dictionary {&#39;rank_abs&#39;:[], &#39;rank_pct&#39;:[], &#39;score&#39;:[]}</span>
<span class="sd">        :param model_id: (Future functionality) ID(s) of models for which to check</span>
<span class="sd">            statistical significance</span>
<span class="sd">        :param attr_cols: Columns indicating attribute values in original_df</span>
<span class="sd">        :param alpha: Level at which to determine statistical significance.</span>
<span class="sd">            Default is 5e-2 (0.05).</span>

<span class="sd">        :return: dataframe with same number of rows as the input (crosstab)</span>
<span class="sd">            but with additional disparity metrics columns, ref_group_values, and</span>
<span class="sd">            statistical significance (p-values) of specific metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s1">&#39;label_value&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Column &#39;label_value&#39; not in dataframe. Label values are &quot;</span>
                <span class="s2">&quot;required for computing statistical significance of supervised &quot;</span>
                <span class="s2">&quot;metrics.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attr_cols</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">non_attr_cols</span> <span class="o">=</span> <span class="p">[</span>
                <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;model_id&#39;</span><span class="p">,</span> <span class="s1">&#39;entity_id&#39;</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">,</span> <span class="s1">&#39;label_value&#39;</span><span class="p">,</span>
                <span class="s1">&#39;rank_abs&#39;</span><span class="p">,</span> <span class="s1">&#39;rank_pct&#39;</span><span class="p">]</span>

            <span class="c1"># index of the columns that are attributes</span>
            <span class="n">attr_cols</span> <span class="o">=</span> <span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="o">~</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">non_attr_cols</span><span class="p">)]</span>

        <span class="c1"># check if all attr_cols exist in df</span>
        <span class="k">if</span> <span class="nb">set</span><span class="p">(</span><span class="n">attr_cols</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Not all attribute columns provided &#39;</span><span class="si">{</span><span class="n">attr_cols</span><span class="si">}</span><span class="s2">&#39; exist in &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;input dataframe.&quot;</span><span class="p">)</span>

        <span class="c1"># check if all columns are strings:</span>
        <span class="n">non_string_cols</span> <span class="o">=</span> \
            <span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span>
                <span class="p">(</span><span class="n">original_df</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">!=</span> <span class="nb">object</span><span class="p">)</span> <span class="o">&amp;</span>
                <span class="p">(</span><span class="n">original_df</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">!=</span> <span class="nb">str</span><span class="p">)</span> <span class="o">&amp;</span>
                <span class="p">(</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">attr_cols</span><span class="p">))]</span>

        <span class="n">binary_inclusions</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s1">&#39;binary_</span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="p">(</span><span class="n">selected_significance</span> <span class="ow">or</span>
                                                         <span class="bp">cls</span><span class="o">.</span><span class="n">significance_cols</span><span class="p">)</span>
                             <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s1">&#39;label_value&#39;</span><span class="p">}</span>

        <span class="c1"># check whether necessary binary columns already created</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">non_string_cols</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
            <span class="c1"># check whether error message is required based on if only binary columns</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="s1">&#39;binary_&#39;</span> <span class="ow">in</span> <span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">non_string_cols</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;get_statistical_significance: statistical significance was &#39;</span>
                    <span class="s1">&#39;not calculated. There are non-string cols within attr_cols.&#39;</span><span class="p">)</span>

            <span class="c1"># skip binary column creation if all necessary columns already created</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">binary_inclusions</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">non_string_cols</span><span class="p">):</span>
                <span class="n">measures</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;binary_&#39;</span><span class="p">)])</span>
                <span class="n">measures</span> <span class="o">=</span> <span class="n">measures</span><span class="o">.</span><span class="n">union</span><span class="p">({</span><span class="s1">&#39;label_value&#39;</span><span class="p">})</span>

                <span class="k">for</span> <span class="n">attribute</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">attr_cols</span><span class="p">)</span> <span class="o">-</span> <span class="n">measures</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">measure</span> <span class="ow">in</span> <span class="n">measures</span><span class="p">:</span>

                        <span class="c1"># only calculate significance if in selected_significance</span>
                        <span class="k">if</span> <span class="p">(</span><span class="n">measure</span> <span class="ow">in</span> <span class="n">selected_significance</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                                <span class="n">measure</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;binary_&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="ow">in</span> <span class="n">selected_significance</span><span class="p">):</span>
                            <span class="bp">cls</span><span class="o">.</span><span class="n">_calculate_significance</span><span class="p">(</span><span class="n">original_df</span><span class="p">,</span> <span class="n">disparity_df</span><span class="p">,</span>
                                                        <span class="n">attribute</span><span class="p">,</span> <span class="n">measure</span><span class="p">,</span> <span class="n">ref_dict</span><span class="o">=</span><span class="n">ref_dict</span><span class="p">,</span>
                                                        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">disparity_df</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># drop old binary columns and generate those in binary_inclusions</span>
                <span class="c1"># as though non_string_cols was empty</span>
                <span class="n">drop_cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;binary_&#39;</span><span class="p">)])</span>
                <span class="n">drop_cols</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;rank_&#39;</span><span class="p">)])</span>
                <span class="n">original_df</span> <span class="o">=</span> <span class="n">original_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">drop_cols</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># create binary columns if non_string_cols is empty or binaries dropped</span>

        <span class="c1"># if no score_thresholds are provided, we assume that rank_abs equals</span>
        <span class="c1"># the number  of 1s in the score column; it also serves as flag to set</span>
        <span class="c1"># parameter to &#39;binary&#39;</span>
        <span class="n">count_ones</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">score_thresholds</span><span class="p">:</span>
            <span class="n">original_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

            <span class="n">count_ones</span> <span class="o">=</span> <span class="n">original_df</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">score_thresholds</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;rank_abs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">count_ones</span><span class="p">]}</span>

        <span class="n">original_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;score&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">original_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;rank_abs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_df</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">original_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;rank_pct&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;rank_abs&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_df</span><span class="p">)</span>

        <span class="c1"># Define formula for binary false positive, false negative, binary score</span>
        <span class="n">binary_false_pos</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">rank_col</span><span class="p">,</span> <span class="n">label_col</span><span class="p">,</span> <span class="n">thres</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">rank_col</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">thres</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">label_col</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">binary_false_neg</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">rank_col</span><span class="p">,</span> <span class="n">label_col</span><span class="p">,</span> <span class="n">thres</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">rank_col</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thres</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">label_col</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">binary_score</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">rank_col</span><span class="p">,</span> <span class="n">label_col</span><span class="p">,</span> <span class="n">thres</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span>
                <span class="n">x</span><span class="p">[</span><span class="n">rank_col</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">thres</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">binary_col_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;binary_score&#39;</span><span class="p">:</span> <span class="n">binary_score</span><span class="p">,</span>
                                <span class="s1">&#39;binary_fpr&#39;</span><span class="p">:</span> <span class="n">binary_false_pos</span><span class="p">,</span>
                                <span class="s1">&#39;binary_fnr&#39;</span><span class="p">:</span> <span class="n">binary_false_neg</span>
                                <span class="p">}</span>

        <span class="k">for</span> <span class="n">thres_unit</span><span class="p">,</span> <span class="n">thres_values</span> <span class="ow">in</span> <span class="n">score_thresholds</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">thres_val</span> <span class="ow">in</span> <span class="n">thres_values</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">binary_col_functions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">func</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">thres_unit</span><span class="p">,</span> <span class="s1">&#39;label_value&#39;</span><span class="p">,</span> <span class="n">thres_val</span><span class="p">)</span>
                    <span class="n">original_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># add columns for error-based significance</span>
        <span class="c1"># precision, tnr, fdr are based on false positives</span>
        <span class="n">SIGNIF_BASES</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;binary_precision&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_fpr&#39;</span><span class="p">,</span>
            <span class="s1">&#39;binary_tnr&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_fpr&#39;</span><span class="p">,</span>
            <span class="s1">&#39;binary_fdr&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_fpr&#39;</span><span class="p">,</span>
            <span class="s1">&#39;binary_npv&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_fnr&#39;</span><span class="p">,</span>
            <span class="s1">&#39;binary_tpr&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_fnr&#39;</span><span class="p">,</span>
            <span class="s1">&#39;binary_for&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_fnr&#39;</span><span class="p">,</span>
            <span class="s1">&#39;binary_ppr&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_score&#39;</span><span class="p">,</span>
            <span class="s1">&#39;binary_pprev&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_score&#39;</span><span class="p">}</span>

        <span class="c1"># add columns for the rest of columns in dictionary keys</span>
        <span class="c1"># binary score, fnr, fpr already added above</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="p">(</span><span class="n">binary_inclusions</span> <span class="o">-</span> <span class="n">binary_col_functions</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="n">original_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">SIGNIF_BASES</span><span class="p">[</span><span class="n">col</span><span class="p">]]</span>

        <span class="c1"># ensure only predicted positive values included in true positive</span>
        <span class="c1"># error based metrics, and only predicted negative values in false</span>
        <span class="c1"># positive error based metrics</span>
        <span class="n">POSITIVE_ONLY</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;binary_fpr&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_tnr&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_precision&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_fdr&#39;</span><span class="p">]</span>
        <span class="n">NEGATIVE_ONLY</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;binary_fnr&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_tpr&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_npv&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_for&#39;</span><span class="p">]</span>

        <span class="n">FPR_BASED</span> <span class="o">=</span> <span class="p">[</span><span class="n">metric</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">POSITIVE_ONLY</span> <span class="k">if</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">binary_inclusions</span><span class="p">]</span>
        <span class="n">FNR_BASED</span> <span class="o">=</span> <span class="p">[</span><span class="n">metric</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">NEGATIVE_ONLY</span> <span class="k">if</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">binary_inclusions</span><span class="p">]</span>

        <span class="n">original_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">original_df</span><span class="p">[</span><span class="s1">&#39;binary_score&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">FPR_BASED</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="n">original_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">original_df</span><span class="p">[</span><span class="s1">&#39;binary_score&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">FNR_BASED</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

        <span class="n">measures</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">original_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;binary_&#39;</span><span class="p">)])</span>
        <span class="n">measures</span> <span class="o">=</span> <span class="n">measures</span><span class="o">.</span><span class="n">union</span><span class="p">({</span><span class="s1">&#39;label_value&#39;</span><span class="p">})</span>

        <span class="k">for</span> <span class="n">attribute</span> <span class="ow">in</span> <span class="n">attr_cols</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">measure</span> <span class="ow">in</span> <span class="n">measures</span><span class="p">:</span>

                <span class="c1"># only calculate significance if in selected_significance</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">measure</span> <span class="ow">in</span> <span class="n">selected_significance</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">measure</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;binary_&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="ow">in</span> <span class="n">selected_significance</span><span class="p">):</span>

                    <span class="bp">cls</span><span class="o">.</span><span class="n">_calculate_significance</span><span class="p">(</span><span class="n">original_df</span><span class="p">,</span> <span class="n">disparity_df</span><span class="p">,</span>
                                                <span class="n">attribute</span><span class="p">,</span> <span class="n">measure</span><span class="p">,</span> <span class="n">ref_dict</span><span class="o">=</span><span class="n">ref_dict</span><span class="p">,</span>
                                                <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">disparity_df</span>

<div class="viewcode-block" id="Bias.list_disparities">
<a class="viewcode-back" href="../../../api/aequitas.html#src.aequitas.bias.Bias.list_disparities">[docs]</a>
    <span class="k">def</span> <span class="nf">list_disparities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        View list of all calculated disparities in df</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;_disparity&#39;</span><span class="p">)])</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;No disparity columns found in dataframe. Tip: &quot;</span>
                           <span class="s2">&quot;make sure you have already passed the dataframe &quot;</span>
                           <span class="s2">&quot;through a &#39;get_disparity_&#39; method.&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="Bias.list_significance">
<a class="viewcode-back" href="../../../api/aequitas.html#src.aequitas.bias.Bias.list_significance">[docs]</a>
    <span class="k">def</span> <span class="nf">list_significance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        View list of all calculated disparities in df</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;_significance&#39;</span><span class="p">)])</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;No significance columns found in dataframe. Tip: &quot;</span>
                           <span class="s2">&quot;make sure you set the &#39;check_significance&#39; parameter&quot;</span>
                           <span class="s2">&quot; to True in &#39;get_disparity_&#39; method(s).&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="Bias.list_absolute_metrics">
<a class="viewcode-back" href="../../../api/aequitas.html#src.aequitas.bias.Bias.list_absolute_metrics">[docs]</a>
    <span class="k">def</span> <span class="nf">list_absolute_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        View list of all calculated absolute bias metrics in df</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_group_metrics</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span></div>
</div>

</pre></div>
    </div></div>
        </main>
      </div>
    </div><footer class="py-6 border-t border-border md:py-0">
    <div class="container flex flex-col items-center justify-between gap-4 md:h-24 md:flex-row">
      <div class="flex flex-col items-center gap-4 px-8 md:flex-row md:gap-2 md:px-0">
        <p class="text-sm leading-loose text-center text-muted-foreground md:text-left">Â© 2024, Center for Data Science and Public Policy&nbsp;Built with <a class="font-medium underline underline-offset-4"
    href="https://www.sphinx-doc.org"
    rel="noreferrer">Sphinx 7.2.6</a></p>
</div>
</div>
</footer>
  </div>
  
    <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script defer="defer" src="../../../_static/theme.js?v=40b7bc71"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
  
</body>
</html>