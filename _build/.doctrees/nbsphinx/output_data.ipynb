{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Output\n",
    "\n",
    "Here we provide a highlevel overview of the bias and fairness metrics output by aequitas. \n",
    "For detailed definitions of the metrics see [Understanding our metrics](./metrics.html)\n",
    "\n",
    "|Output type | report | csv |\n",
    "| ---| :---: | :---: |\n",
    "|High-level overview | Yes | No |\n",
    "| Fairness | Yes | Yes |\n",
    "| Disparity | Yes | Yes|\n",
    "|Group-level metrics | Yes |Yes|\n",
    "\n",
    "You can get Aequitas output in report form via the webapp or CLI as well as a csv/DataFrame using the CLI or Python. Below we reproduce the tables found in the output using \n",
    "\n",
    "## Fairness overview\n",
    "\n",
    "The webapp and pdf report begins with a high level analysis of fairness. If all fairness metrics are `fair`, The Bias Report will evalutate the current model as fair. Otherwise, it will deem it unfair and list groups unfairly affected under given fairness criteria.\n",
    "![](_static/overview_adult_output.jpg)\n",
    "\n",
    "In the above example, we see the model has Proportional Parity and False Positive Parity, but does not meet the False Negative Parity criteria for the race Amer-Indian-Eskimo and does not have Equal Parity for many race groups. On the webapp, you can click on the names of the fairness criteria for explanations of the criteria, an example of why you care and more detailed statistics explaining why a group does not have parity.\n",
    "\n",
    "## Fairness Criteria Assessments\n",
    "\n",
    "Fairness is defined in relation to a reference group. In the Fairness Criteria Assessments, a group meets parity if\n",
    "\n",
    "$$ (1 - \\tau) \\leq Disparity Measure_{group_i} \\leq \\frac{1}{(1 - \\tau)}$$\n",
    "\n",
    "where $\\tau$ is the fairness threshold defined in the webapp.\n",
    "\n",
    "![](_static/fairness_adult_output.jpg \"fairness\")\n",
    "\n",
    "In our example $\\tau = 20\\%$, so any disparity measure between 0.8 and 1.25 will be deemed fair. (This is inline with the 80 percent rule for determining [disparate impact](https://en.wikipedia.org/wiki/Disparate_impact)).\n",
    "\n",
    "Clicking on the word fair or unfair will bring you to the next table.\n",
    "\n",
    "## Disparity and Bias Metrics\n",
    "\n",
    "Above Fairness was determined by the size of a disparity measure. In this table, you see the disparity measures value.\n",
    "Disparity is a ratio of a groups metric compared to a reference group. Notice the reference group will always have disparity of 1.\n",
    "\n",
    "![](_static/disparity_adult_output.jpg \"disparity\")\n",
    "\n",
    "From the high-level overview, we saw that many groups did not have Equal or Statistical Parity. Here we see that the ratio of those groups Predicted Postive Rates compared to the reference group are very low. \n",
    "\n",
    "\n",
    "## Group Metrics\n",
    "\n",
    "The disparities above are derived from group level metrics. \n",
    "![](_static/group_adult_output.jpg \"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the False Positive Rate Disparity for black people compared to white people is calculated as follows.\n",
    "\n",
    "$$ FPR Disparity_{black} = \\frac{FPR_{black}}{FPR_{white}} = \\frac{.95}{.9} = 1.06 $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
